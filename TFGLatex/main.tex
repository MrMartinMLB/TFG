\documentclass{report}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[sorting=none]{biblatex}
\bibliography{bibliografia}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    citecolor = magenta,      
    urlcolor=cyan,
    }
\usepackage{amsmath, amssymb,graphics,setspace,vmargin,graphicx,color,multirow,wrapfig}
\usepackage[spanish,es-tabla]{babel}
\usepackage{csquotes}
\usepackage{float} % para usar [H]

\usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl,pdfpages}
\usepackage[font=normalsize,labelfont=bf]{caption}
\usepackage{circuitikz}
\usepackage{adjustbox}
\usepackage{lscape}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{pdfpages}
\usepackage{multimedia}
\usepackage{media9}
\usepackage{pythonhighlight}
\usepackage[margin=1in]{geometry}
\usepackage{afterpage}
%\usepackage[backend=biber,sorting=nyt]{biblatex}

%\usepackage[nottoc,numbib]{tocbibind}

%\usepackage[nottoc,numbib]{tocbibind}
%\usepackage[table,xcdraw]{xcolor}

%\usepackage[backend=bibtex]{biblatex}
%\addbibresource{bibliografia.bib}

% Tipo de cabecera de capitulo
%Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, Bjornstrup
\usepackage[Glenn]{fncychap}

%Directorio de figuras
\graphicspath{ {FIGURAS/} } 



%Tipo de fuente
\usepackage{palatino}

%Notas de cabecera
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\leftmark} \chead{} \rhead{}
\lfoot{} \cfoot{\thepage} \rfoot{}



% Unidades del sistema internacional
\usepackage{siunitx}
% Para usar algo en SI: \[5\ \si{kg.m/s^2}\]


\newcommand{\mathsym}[1]{{}}
\newcommand{\unicode}[1]{{}}
\setlength{\parskip}{6mm}
\setmargins{2.5cm}       % margen izquierdo
{1.3cm}                  % margen superior
{15.5cm}                 % anchura del texto
{23.42cm}                % altura del texto
{10pt}                   % altura de los encabezados
{1cm}                    % espacio entre el texto y los encabezados
{0pt}                    % altura del pie de página
{2cm}                    % espacio entre el texto y el pie de página

\newcounter{mathematicapage}

\usepackage{rotating}
\usepackage{tikz}


\begin{document}

	% PORTADA
\begin{titlepage}

 \begin{center}
\begin{figure}[hbpt]
    \vspace*{-1cm}
    \centering
    \includegraphics[scale=0.57]{Imagenes/logoUMAeii.png}
    \label{fig:my_label}
\end{figure}
\vspace*{4cm}
 
%\begin{figure}[htdp]
%\centering
%\begin{minipage}[t]{.45\linewidth}
%\centering
%\includegraphics[width=2in]{Imagenes/logoUMA.jpg}%
%\end{minipage}
%\hspace{0.5in}
%\begin{minipage}[t]{0.45\linewidth}
%\centering
%\includegraphics[width=2in]{Imagenes/LogoEII.png}
%\end{minipage}
%\end{figure}

\vspace*{-2.9cm}	%Para meter un espacio
%\huge\textbf{UNIVERSIDAD DE MÁLAGA}
\Large\textbf{ESCUELA DE INGENIERÍAS INDUSTRIALES}

\vspace*{0.4cm}
\huge\textbf{TRABAJO FIN DE GRADO}

\vspace*{0.2cm}
\Large
Departamento de Lenguajes y Ciencias de la Computación
\vspace*{0cm}	

\begin{Large}
Grado en Ingeniería en Tecnologías Industriales

Mención en Itinerario de Electrónica
\end{Large}



\vspace*{-0cm}	

     \rule{\linewidth}{0.2 mm} \\[0.4 cm]
    
   
    \LARGE{Detección automática con redes neuronales profundas de artefactos en superficies fabricadas mediante manufactura aditiva} \\
    \rule{\linewidth}{0.1 mm} \\[0.4 cm]
   \large{Automated inspection of artifact surfaces fabricated by additive manufacturing images by deep convolutional neural networks}\\ 
    
    \rule{\linewidth}{0.2 mm} \\[1 cm]
    \vspace{-0.5cm}


\vspace*{1cm}	


\begin{table}[H]
\centering
\begin{large}

\begin{tabular}{lll}

Realizado por
                \\

\textbf{Martín Loring Bueno}      \\

\\

Tutorizado por
                \\

\textbf{Ezequiel López Rubio} \\


\textbf{Iván Gómez Gallego}     \\

\\



\end{tabular}
\end{large}
\end{table}

\vspace*{0.1cm}

\begin{large}
MÁLAGA, JUNIO DE 2023 
\end{large}
\end{center}

\end{titlepage}
			
			
			





\setcounter{tocdepth}{4}
\setcounter{page}{1}		


\thispagestyle{empty}
\begin{center}
    {\huge  \textbf{Declaración de originalidad} \par}
\end{center}

Yo, don Martín Loring Bueno, estudiante del Grado de Ingeniería en Tecnologías Industriales en la Escuela de Ingenierías Industriales de la Universidad de Málaga, declaro:

Ser autor del Trabajo de Fin de Grado titulado \textbf{''Detección automática con redes neuronales profundas de artefactos en superficies fabricadas mediante
manufactura aditiva''}, tutorizado por Dr. Ezequiel López Rubio y Dr. Iván Gómez Gallego, y que no ha sido presentado con anterioridad, ni total ni parcialmente, para superar materias previamente cursadas en esta u otras titulaciones de la Universidad de Málaga o cualquier otra institución de educación superior u otro tipo de fin. 

Así mismo, declaro no haber trasgredido ninguna norma universitaria con respecto al plagio ni a las leyes establecidas que protegen la propiedad intelectual, así como que las fuentes utilizadas han sido citadas adecuadamente.

\begin{flushright}
Málaga, a 6 de junio de 2023

    Fdo.:
    
    \begin{minipage}[b]{0.23\linewidth}
     \includegraphics[width=1\linewidth]{Imagenes/Signature (1).png} 
    \end{minipage}
    
    Martín Loring Bueno
    
\end{flushright}




\newpage
\thispagestyle{empty}
\begin{center}
    {\huge  \textbf{Resumen} \par}
\end{center}


La impresión 3D, o manufactura aditiva, se ha convertido en una de las revoluciones más importantes para la ingeniería en los últimos años, llegando a ser un pilar fundamental en la de denominada 4ª Revolución Industrial. Este tipo de fabricación, que se basa en la creación de objetos tridimensionales a partir de un modelo digital, no solo ha provocado un gran cambio en la industria, sino que ha permitido que usuarios de todo el mundo tengan acceso a la fabricación de objetos complejos de calidad.

El modelado por deposición fundida (FDM), también conocido como fabricación por filamento fundido (FFF), es el tipo de tecnología de manufactura aditiva actualmente más usado a nivel de usuario. Sin embargo, esta tecnología sufre excesivamente de una falta de control de calidad y regulación durante el proceso, lo que se tratará de solventar en este trabajo mediante el aprendizaje profundo.

El aprendizaje profundo, por su parte, se ha convertido en la técnica dentro de la inteligencia artificial con un mayor respaldo, al ser capaz de obtener resultados que han sido todo un hito en este campo. Gran parte de la responsabilidad del ingeniero al trabajar con estos recursos es ser capaz de encontrar un problema donde las cualidades del aprendizaje profundo puedan brindar soluciones imposibles por otros métodos.

En este contexto, se tratará de aplicar el aprendizaje profundo a la detección de artefactos, o defectos, en capas impresas por fabricación por filamento fundido. En concreto, se entrenará una red neuronal convolucional con un conjunto de datos formado por imágenes de figuras fabricadas por este método.

La mayor parte de este trabajo se centrará en experimentos referidos a este entrenamiento, en los que se variarán diferentes parámetros y se probaran diferentes modelos buscando la mejor respuesta posible. Todos los conceptos teóricos aplicados se explican en este texto, así como la motivación para cada decisión tomada en el proceso.

Finalmente, se encontrará un modelo óptimo que sea capaz de detectar estos defectos en capas de figuras fabricadas por filamento fundido, evidenciando de esta forma la competencia de las redes neuronales profundas en este campo y el posible futuro de modelos de este tipo.




\textbf{Palabras clave:} Inteligencia Artificial, Aprendizaje Profundo, Redes Neuronales Convolucionales, Visión por Computador, Manufactura aditiva, Modelado por deposición fundida (FDM), Post Procesamiento, Detección de Defectos, PyTorch.


\newpage
\thispagestyle{empty}
\begin{center}
    {\huge  \textbf{Abstract} \par}
\end{center}


3D printing, or additive manufacturing, has become one of the most significant revolutions in engineering in recent years, becoming a fundamental pillar in the so-called 4th industrial revolution. This type of manufacturing, which involves creating three-dimensional objects from a digital model, has not only brought about a major change in the industry but has also allowed users from around the world to access the production of complex and high-quality objects.

Fused Deposition Modeling (FDM), also known as fused filament fabrication (FFF), is currently the most widely used additive manufacturing technology at the user level. However, this technology suffers from a lack of quality control and regulation during the process, which will be addressed in this work through deep learning.

Deep learning, on the other hand, has become the most supported artificial intelligence technique after achieving groundbreaking results in this field. A significant responsibility for engineers working with these resources is the ability to find problems where the qualities of deep learning can provide solutions that are otherwise impossible to achieve.

In this context, the aim is to apply deep learning for the detection of artifacts, or defects, in layers printed using Fused Filament Fabrication. Specifically, a convolutional neural network will be trained using a dataset consisting of images of objects manufactured using this method.

The majority of this work will focus on experiments related to this training, where different parameters will be varied, and different models will be tested in search of the best possible performance. All the applied theoretical concepts are explained in this text, as well as the motivation behind each decision made in the process.

Finally, an optimal model will be found that is capable of detecting these defects in layers of objects manufactured using fused filament fabrication, thus demonstrating the competence of deep neural networks in this field and the potential future of such models.


\textbf{Keywords:} Artificial Intelligence, Deep Learning, Convolutional Neural Networks, Computer Vision, Additive Manufacturing, Fused Deposition Modeling (FDM), Post Processing, Detection of Defects, PyTorch.




\tableofcontents
\listoffigures



















\chapter{Introducción}



\section{Motivación y contexto}
La manufactura aditiva, también conocida como impresión 3D, es un proceso de fabricación que construye objetos capa por capa, utilizando información digital. La tecnología de impresión 3D se ha convertido en una herramienta popular para la fabricación de componentes de alta calidad en diversos campos, incluyendo la aeroespacial, la automotriz y la biomédica. Sin embargo, la naturaleza de este proceso de fabricación también puede generar ciertos problemas, como la aparición de artefactos o defectos en la superficie de los objetos fabricados.

La detección automática de estos artefactos o defectos es una tarea importante, ya que puede afectar la calidad y la durabilidad del producto final. En los procesos de fabricación tradicionales, los operarios son responsables de inspeccionar visualmente los objetos para detectar cualquier anomalía. Sin embargo, esta tarea puede ser tediosa y propensa a errores. Por lo tanto, es necesario encontrar soluciones más eficientes y precisas para la detección automática de artefactos.

En los últimos años, las redes neuronales profundas se han utilizado con éxito en la detección de objetos y en la visión por computadora en general. Las redes neuronales profundas son una clase de algoritmos de aprendizaje automático que imitan el funcionamiento del cerebro humano para procesar información y extraer características relevantes de los datos de entrada. En particular, las redes neuronales convolucionales han demostrado ser muy efectivas en la detección de objetos en imágenes.

En este trabajo, se propone el uso de redes neuronales profundas para la detección automática de artefactos en superficies fabricadas mediante manufactura aditiva. Se utilizará un conjunto de datos de superficies fabricadas con diferentes parámetros de impresión para entrenar y evaluar el modelo propuesto. El objetivo es diseñar un modelo de detección de artefactos preciso y eficiente que pueda ser utilizado en la fabricación aditiva para mejorar la calidad de los productos finales.


\section{Objetivos y recursos}

El objetivo principal de este proyecto es desarrollar un sistema de detección automática de artefactos en la superficie de las piezas fabricadas mediante manufactura aditiva utilizando redes neuronales profundas. Específicamente, el proyecto se centrará en la detección de cuatro tipos de artefactos: \textit{Under} (Falta de material), \textit{Over} (Sobra material), \textit{Empty} (Zona vacía) y \textit{Ok} (Zona con el material suficiente). Además, se explorarán diferentes técnicas y algoritmos de aprendizaje profundo para mejorar la precisión de la detección y se realizará una evaluación exhaustiva del rendimiento del sistema propuesto.

Para lograr este objetivo, se utilizarán los siguientes recursos:

\begin{itemize}
    \item Python: el programa se ha realizado en su totalidad en el lenguaje de programación Python.
    \begin{itemize}
        \item PyTorch: se trata de una biblioteca de aprendizaje automático, que posee gran cantidad de recursos para la Inteligencia Artificial (IA).
        \item Matplotlib: biblioteca que permite crear gráficos a partir de datos contenidos en listas o arrays con extensión de NumPy.
        \item Scikit-learn: biblioteca que ofrece recursos para IA.
        \item Time, OS y Copy: bibliotecas empleadas para diversas soluciones en Python. 
        \item NumPy: biblioteca matemática que permite crear vectores y matrices, junto con un gran número de funciones matemáticas.
    \end{itemize}
    \item Google Colaboratory: permite la realización de código Python y su ejecución gracias a las GPUs de Google.
    \item GitHub: todos los códigos creados se pueden encontrar en esta plataforma.
\end{itemize}

\section{Metodología}

En este apartado se indican los pasos seguidos por el alumno para la realización de este trabajo; tanto previos, como durante los experimentos, y tras estos.

\begin{enumerate}
    \item Aprendizaje previo: se ha realizado un aprendizaje del lenguaje Python por parte del alumno, así como de la biblioteca de inteligencia artificial PyTorch y de conceptos de inteligencia artificial y redes neuronales.
    \item Planteamiento de los objetivos: se ha encontrado un problema en el cual la inteligencia artificial podría encontrar soluciones mejores, como es la detección de artefactos de superficies fabricadas mediante manufactura aditiva.
    \item Investigación: se ha realizado un amplio análisis de la manufactura aditiva y la fabricación por filamento fundido, así como de las redes neuronales convolucionales para clasificación de imágenes.
    \item Realización de experimentos: se han realizado numerosos experimentos, probando el rendimiento de diferentes modelos y con diferentes parámetros aplicados, hasta llegar a un resultado considerado aceptable.
    \item Documentación: se ha redactado este documento, donde se reflejan los conceptos relevantes a la manufacturación aditiva y la inteligencia artificial. Debido a los contenidos técnicos de este trabajo, muchos no relacionados con los grados de la escuela de ingenierías industriales, se ha dedicado una gran parte del documento a la explicación de conceptos necesarios para este trabajo. Se ha explicado también el \textit{dataset} utilizado, así como las técnicas aplicadas en el código. Por último, se han reflejado los resultados que han llevado a un modelo óptimo mediante diferentes gráficas, y se ha concluido con un resumen y posibles mejoras futuras.
\end{enumerate}



\section{Estructura de la memoria}

El proyecto se divide en los siguientes apartados para lograr la mayor compresión y contextualización de las técnicas empleadas:

\begin{enumerate}
    \item Introducción: Se comenzará mediante la introducción de los conceptos principales utilizados en este proyecto: manufactura aditiva y aprendizaje profundo, se ofrecerá la motivación y el contexto en el que ocurre este trabajo, explicando las partes en que se divide la memoria, así como la metodología y recursos utilizados.
    \item Estado del arte. 
    \begin{itemize}
        \item  Manufactura aditiva: Se detallarán los conceptos necesarios para el entendimiento de la manufactura aditiva, así como su relevancia actual, centralizando en los problemas existentes para la fabricación por filamento fundido y posibles soluciones.
        \item Inteligencia artificial: Se diferenciarán y explicarán conceptos como inteligencia artificial, aprendizaje automático y redes neuronales, indagando en las redes neuronales convolucionales.
    \end{itemize}  
    \item Datasets: Se expondrá el conjunto de datos empleado.
    \item Metodología: Se explicarán todos los conceptos y técnicas utilizadas durante el trabajo, de forma que los experimentos, decisiones y conclusiones tengan una mayor comprensión.
    \item Desarrollo de la solución y resultados obtenidos: Se expondrán los resultados obtenidos para los diferentes parámetros y modelos, explicando los conceptos tras los resultados obtenidos y las medidas tomadas.
    \item Conclusiones y líneas futuras: Se resumirán los resultados obtenidos y su importancia. Tras lo que se expondrán posibles mejoras, así como otros posibles proyectos que sigan la línea de investigación de este.
    \item Bibliografía y Apéndices: Finalmente, se detallará en la bibliografía los documentos consultados durante el desarrollo del trabajo para la compresión de conceptos y redacción del código. En el apéndice se mostrará parte del código empleado, y se referenciará al resto de los códigos disponibles en GitHub.
\end{enumerate}































\chapter{Manufactura aditiva}\label{ManufAditiv}

\section{Introducción}

La manufactura aditiva se define, según la Organización Internacional de Normalización (ISO) \cite{AMISOdefinicion}, como ``el término general para referirse a todas las tecnologías que utilizan el proceso de juntar materiales para construir objetos físicos basándose en datos de modelos 3D generados por ordenador, al contrario que las metodologías de fabricación sustractiva y formativa''.

Este término, conocido coloquialmente como impresión 3D, abarca las tecnologías que fabrican objetos en tres dimensiones, añadiendo sucesivamente las capas que forman el mismo. Los objetos son modelados mediante planos virtuales realizados por diseño asistido por ordenador (CAD) o por software de modelado y animación, que la máquina utiliza como guía para la impresión. \cite{AMWikiDefinition} \cite{AMNoWikiDefinition40Industry}


\vspace{0.6cm}
\begin{figure}[hbpt]
    \centering
    \includegraphics[scale=0.5]{Imagenes/AM/AMEjemploCapas.png}
    \caption{La manufactura aditiva o impresión 3D produce objetos capa por capa mediante modelos 3D generados por ordenador}
    \label{fig:Impresion3D}
\end{figure}
\vspace{1.5cm}

\vspace{0.4cm}
Desde la introducción e implementación de la manufactura aditiva a mediados y finales de los 80 por parte de la empresa estadounidense 3D Systems \cite{AMInvention}, se han desarrollado numerosos avances. Esto ha permitido desarrollar diferentes técnicas y el uso de ellas en un amplio campo de modalidades, como son la industria automovilística \cite{AMinAuto} y la aeroespacial \cite{AMinAero} o la medicina \cite{AMinMedicine}. Asimismo, los avances más revolucionarios en los últimos años han traído resultados fascinantes, como la bioimpresión de diferentes tipos de tejidos y órganos \cite{AMInnovativeOrgans}, o la impresión 3D de aerogeles de grafeno, materiales con características óptimas para la ingeniería \cite{AMInnovativeGrafeno}. En la Figura \ref{fig:PelotaNBA} se puede encontrar otro ejemplo de las ventajas de esta tecnología fuera del ámbito industrial y académico. Se trata de un prototipo de balón sin necesidad de ser inflado, desarrollado por Wilson, proveedor oficial de la NBA \cite{AMInnovativeNBA}. 


\vspace{0.4cm}
\begin{figure}[hbpt]
    \centering
    \includegraphics[scale=0.9]{Imagenes/AM/PelotaNBA.png}
    \caption{Prototipo de balón utilizando manufactura aditiva \cite{AMInnovativeNBA}}
    \label{fig:PelotaNBA}
\end{figure}
\vspace{0.4cm}


La propia naturaleza de este tipo de fabricación conlleva una serie de ventajas frente a otros métodos tradicionales. La manufactura se puede llevar a cabo bajo demanda, sin depender de otras partes manufacturadas en otros lugares, ni existir la necesidad de un gran inventario. La producción se puede realizar en pequeños lotes, existiendo además un alto grado de personificación del producto, lo que es espacialmente relevante en campos como la medicina o la biología. La tecnología no necesita una gran inversión de recursos, generando pocos desechos, lo que facilita la posibilidad de manufactura entre usuarios no profesionales. \cite{AMGoodArticle}

Es considerada por gran parte de la comunidad investigadora como un avance revolucionario en el sector de la manufactura, al ser capaz de encontrar soluciones para problemas sin resolver por parte de las tecnologías convencionales \cite{AMRevolution}. Además, la impresión 3D es considerada uno de los componentes fundamentales de la 4ª Revolución Industrial o Industria 4.0, puesto que ambas conllevan autonomía e interconectividad entre empleados, máquinas, proveedores y consumidores. Por último, la impresión 3D supone un avance al ser un método de manufactura con mayor sostenibilidad, lo que podría mitigar los efectos medioambientales de la industrialización. \cite{AMNoWikiDefinition40Industry}


\section{Tipología}

Entre las diferentes técnicas de manufactura aditiva, la Organización Internacional de Normalización (ISO), diferencia siete categorías principales \cite{AMTipos}:

\begin{itemize}
\item Extrusión: en el cual se empuja un filamento de material termoplástico sólido a través de una boquilla calentada, derritiéndolo. Dentro de esta categoría, se encuentra el modelado por deposición fundida (FDM), también denominado fabricación con filamento fundido (FFF).


\item Polimerización VAT: una resina de fotopolímero en una tina es curada selectivamente mediante una fuente de luz. Las dos formas más comunes de polimerización de depósitos son: SLA (estereolitografía) y DLP (Procesamiento de luz digital).

\item Fusión en lecho de polvo: utiliza una fuente de calor (normalmente un láser) para sinterizar o fusionar las partículas de polvo atomizado. Algunos ejemplos son el sinterizado selectivo por láser (SLS) o la fusión por haz de electrones (EBM).

\item Inyección: se inyectan capas de un fotopolímero líquido en una bandeja de impresión y las endurecen instantáneamente usando luz ultravioleta. Las principales tecnologías utilizadas son la inyección de material (MJ) o la DOD, en inglés \textit{Drop on Demand} (gota a demanda).

\item Inyección de aglutinante: conocida como \textit{Binder Jetting} (BJ), en este proceso un cabezal de impresión a chorro se desplaza sobre un lecho de polvo, depositando selectivamente un líquido aglutinante, y repite este proceso hasta formar la pieza completa.

\item Deposición de energía directa (DED): se introduce el material y se fusiona mediante una potente energía térmica aplicada al mismo tiempo que este se deposita. Normalmente, esta energía proviene de un láser o de un haz de electrones.

\item Laminación: se apilan y laminan hojas de materiales mu finos para generar objetos. Estas capas se pueden fusionar utilizando diversos métodos, siendo el calor y el sonido los principales.

\end{itemize}

 
\subsection{Modelado por deposición fundida (FDM)}

Si bien el concepto de identificación de desperfectos desarrollado en este trabajo es aplicable a todos los tipos de manufactura aditiva, el conjunto de datos utilizado será de un proceso de fabricación utilizando FDM. 

Como se expone en el apartado anterior, el modelado por deposición fundida (FDM) es un tipo de impresión 3D que utiliza la técnica de extrusión. Esta tecnología fue desarrollada y patentada por la empresa israelí-estadounidense Stratasys en 1989. En 2009, esta patente expiró, dando lugar al uso extendido de una tecnología muy similar conocida con el término de fabricación con filamento fundido (FFF). Tras esto, la impresión 3D ha sido accesible para una gran parte de la población mundial, provocando un gran interés mediático en 2014 \cite{AMFFFHistory}. Por esto, los términos de modelado por deposición fundida (FDM) y filamento fundido (FFF) se suelen intercalar y usar como sinónimos, si bien existe una pequeña diferencia entre estos, y es la falta de un entorno de impresión calentado en FFF. La cámara calentada de FDM ayuda a controlar la temperatura de la pieza y reducir las tensiones residuales en el producto terminado, mientras que la temperatura no controlada en las máquinas FFF hace que sus resultados sean menos precisos y más propensos a deformarse \cite{AMFFFDiference}.
A lo largo de este texto, ambos términos se usarán con el mismo significado.

El modelado por deposición fundida (FDM), es la técnica más económica y puede usarse con una gran variedad de materiales, lo que le ha permitido ser la técnica más ampliamente utilizada y sobre la que existe una mayor investigación. Sin embargo, su principal obstáculo es su falta de precisión dimensional, al sufrir gravemente de una falta de evaluación de calidad en línea y ajuste de proceso \cite{AMDataset}. 

La existencia de estas limitaciones en la impresión, junto con la gran comunidad creada en los últimos años de aficionados a esta tecnología, tanto a nivel de usuario como de investigadores, hace que este tipo de impresión sea óptima para el reconocimiento planteado en este trabajo.


\subsubsection{Proceso de fabricación}

El fenómeno físico que ocurre durante el modelado por deposición fundida (FDM) se puede dividir en tres fases \cite{AMFFFProcess}: el flujo de material pasa a través de la boquilla y se deposita en la cama de impresión o en otras capas de material ya depositado (extrusión), los gránulos depositados interaccionan creando una unión (fusión) y el material se enfría (solidificación).


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{Imagenes/AM/FFFPhysics.png}
    \caption{Diagrama esquemático del proceso de fabricación mediante FFF \cite{AMFFFProcess}}
    \label{fig:FFFPhysics}
\end{figure}
\vspace{0.4cm}


\subsubsection{Limitaciones y mejoras}

La principal desventaja del modelado FDM es su falta de precisión
dimensional, al sufrir gravemente de una falta de evaluación de calidad en línea y ajuste de proceso. Esta falta de calidad en los acabados de las piezas puede ser solucionada con un postprocesamiento o acabado de las piezas.

El tipo de acabado necesario varía según el tipo de fabricación de la cual proceda la pieza. En el caso del modelado por deposición fundida, es común el uso de lijado y el llenado de huecos \cite{AMFFFPostProcessing}. La detección de defectos necesaria para tratar las piezas suele ser una tarea manual, que requiere una gran pérdida de tiempo y cuya complejidad puede provocar que el técnico que realiza este trabajo no lo haga con una gran efectividad.

Este trabajo propone solucionar esta cuestión utilizando la inteligencia artificial y, en concreto, mediante redes neuronales profundas (explicadas en el Apartado \ref{IA}), que sean capaces de aprender a distinguir las zonas con defectos de un conjunto de imágenes. El conjunto de imágenes o \textit{dataset}, que se explica en profundidad en el Apartado \ref{section:Dataset}, está formado por imágenes de capas resultantes tras una impresión 3D. Estas imágenes han sido convertidas en imágenes 2D, convirtiendo la altura a diferentes colores. La red neuronal convolucional deberá ser capaz de distinguir las partes de la pieza que necesiten lijado o rellenado de huecos.





























\chapter{Inteligencia Artificial (IA)}\label{IA}



\section{Breve contexto histórico}\label{HistoriaIA}

La inteligencia artificial (IA) es un campo interdisciplinario de la ciencia que se centra en la creación de sistemas que pueden realizar tareas que normalmente requerirían inteligencia humana, como el aprendizaje, la percepción, el razonamiento y la toma de decisiones. El término  ```inteligencia artificial'' fue acuñado por John McCarthy en 1956 durante la Conferencia de Dartmouth, donde se reunieron algunos de los primeros investigadores de IA \cite{JohnMccarthy}.

Los inicios de la IA se remontan a la década de 1940, cuando se empezó a investigar cómo construir máquinas que pudieran realizar tareas que requerían inteligencia humana. En 1950 Alan Turing publicó el artículo \textit{Computing Machinery and Intelligence} \cite{AlanTuring}, en el que introducía el test de Turing para diferenciar un humano de una máquina. Uno de los primeros éxitos de la IA fue en este mismo año con el programa de ajedrez de Claude Shannon \cite{ClaudeShannon}. A partir de entonces, la IA experimentó un rápido crecimiento en la década de 1950 y 1960, con el desarrollo de los primeros programas de lógica simbólica y el descubrimiento del algoritmo de aprendizaje automático conocido como "perceptrón", considerada la primera red neuronal artificial. En la Figura \ref{fig:perceptron} se aprecia una un esquema perceptrón incluido en \textit{“The Design of an Intelligent Automaton,”}, 1958.


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{Imagenes/IA/rosenblatt_nuron.jpg}
    \caption{ Esquema del perceptrón, primera red neuronal introducida por Frank Rosenblatt en 1958 }
    \label{fig:perceptron}
\end{figure}
\vspace{0.4cm}


Sin embargo, en la década de 1970 se produjo un "invierno de la IA", ya que los investigadores se dieron cuenta de que las técnicas existentes eran limitadas y no podían cumplir con las expectativas iniciales. Esta época fue especialmente provocada por la publicación del libro \textit{``Perceptrons an introduction to computational geometry''} de Minsky y Papert \cite{LibroInviernoIA}, el cual explicaba los problemas para resolver los cálculos de las redes neuronales formadas por el ``perceptrón''. Durante este período, la financiación de la investigación en IA disminuyó significativamente.

En 1986, el interés a la inteligencia artificial vuelve tras la publicación del artículo \textit{``Learning representations by back-propagating errors''} \cite{LibroFinInvierno}, en el cual se introduce el concepto de \textit{back-propagation} como la técnica matemática que solucionaría los problemas de cálculo de las redes neuronales. En esta década, se introdujeron los ``sistemas expertos'', que agrupaban grandes datos de conocimiento para resolver diversos problemas como si se tratara de un humano. Sin embargo, la necesidad de un gran coste para el complejo hardware provoco un segundo ``invierno'' hasta entrados los 2000 \cite{SegundoInviernoIA}.

El interés resurgió cuando en 1996, el ordenador de IBM Deep Blue venció al campeón del mundo de ajedrez Garry Kasparov \cite{KasparovDeepBlue}, siendo televisado con gran expectación. Esta máquina utilizada ``fuerza bruta'' y no aprendía el juego como lo harían otras en el futuro.

En la década de los 2010, se produjo una gran revolución por parte de las redes neuronales, las cuales están formadas por neuronas, como ya imaginó John McCarthy. Estas redes son capaces de aprender en el sentido más humano de la palabra, a través de la exposición a un tipo de dato, extrayendo características por sí solas y clasificándolas. En 2012, Jeff Dean y Andrew Ng utilizaron los servidores de Google para construir una gran red neuronal que fue alimentada con 10 millones de imágenes de YouTube. La red fue programada para reconocer patrones y pudo extraer tres: una cara y un cuerpo humano, y un gato \cite{JeffanAndrew}. Más tarde, en 2016, los científicos de DeepMind consiguieron ganar al campeón del mundo del juego japonés de estrategia Go con la IA AplhaGo \cite{AlphaGo}. Estos mismos científicos desarrollaron AlphaFold, otra IA capaz de resolver el problema de predecir la estructura 3D de una proteína a través de su secuencia de aminoácidos \cite{AlphaFold}. Este último supone un hito histórico que hizo ver la capacidad de la inteligencia artificial y el aprendizaje profundo en numerosos campos del conocimiento.

\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Imagenes/IA/AlphaGo.png}
    \caption{ La red neuronal profunda AlphaGo gana 3-1 al campeón del mundo de go \cite{AlphaGoGuardian} }
    \label{fig:AlhaGO}
\end{figure}
\vspace{0.4cm}



En la actualidad, la inteligencia artificial es un término de uso constante y cuya utilidad es aplicada en prácticamente todos los campos de la ciencia y la ingeniería. Con la reciente introducción de ChatGPT-3 en noviembre de 2022 por parte de la empresa OpenAI \cite{ChatGPT}, la inteligencia artificial ha causado un gran ruido mediático, y un cambio revolucionario en la accesibilidad de estas herramientas al público general, siendo aún pronto para poder medir el impacto de estas herramientas en la sociedad.




\section{Inteligencia artificial, \textit{machine learning} y \textit{deep learning} }


Con el boom de la inteligencia artificial, es habitual escuchar mucha discusión al respecto y, en muchos casos, por parte de personas no entendidas en el campo. Esto provoca que términos que definen diferentes conceptos como inteligencia artificial, \textit{machine learning}, o aprendizaje automático, y \textit{deep learning}, o aprendizaje profundo, se usen como sinónimos y se intercambien. 


En el apartado anterior se definió la inteligencia artificial como el campo interdisciplinario de la ciencia que se centra en la creación de sistemas que pueden realizar tareas que normalmente requerirían inteligencia humana, como el aprendizaje, la percepción, el razonamiento y la toma de decisiones. Es decir, que entrarían en este conjunto variedad de sistemas que realizan acciones inteligentes, como los que existen en robótica o aquellos que se utilizan para procesamiento del lenguaje.


El aprendizaje automático o \textit{machine learning}, por otro lado, se trata de un campo dentro de la inteligencia artificial que se refiere al estudio de sistemas informáticos que aprenden y se adaptan automáticamente a partir de la experiencia sin ser programados explícitamente. Mientras que dentro de la inteligencia artificial pueden existir máquinas completamente programadas por parte de un humano para responder de una u otra forma, es decir, sin una verdadera capacidad para razonar o aprender, en el aprendizaje automático es el propio modelo el que, a través de un algoritmo, puede procesar grandes cantidades de datos y tomar la decisión por sí mismo. Un ejemplo de este campo sería la recomendación directa de música por parte de aplicaciones musicales como Spotify, que tras estudiar tus gustos y los de otras personas con gustos parecidos, es capaz de recomendar canciones que podrían gustar a la persona \cite{Coursera}.


La técnica dentro del aprendizaje automático que ha tomado un papel protagonista en los últimos años por sus resultados son las redes neuronales o \textit{neural networks}, las cuales son capaces de aprender de forma jerarquizadas, es decir, aprenden por niveles o capas. Las primeras capas, más cercanas a la información de entrada, aprenden conceptos básicos, como texturas, colores o formas. Por otro lado, las capas más cercanas a la salida aprenden conceptos más complejos al ir alejándose, hasta llegar a ser capaces de reconocer objetos altamente complicados, como coches, caras o animales \cite{VideoCSV}.


Por último, el aprendizaje profundo o \textit{deep learning} se trata de un subconjunto del aprendizaje automático formado por redes neuronales de 3 o más capas \cite{IBMEspañol}. Mientras que los algoritmos de aprendizaje automático necesitan datos procesados anteriormente por humanos o la corrección ante el fallo, los algoritmos de aprendizaje profundo mejoran su aprendizaje mediante la repetición, sin intervención humana. Los algoritmos de aprendizaje profundo necesitan grandes cantidades de datos para poder realizar predicciones, pero a diferencia de otros aprendizajes automáticos, son capaces de ``digerir'' y procesar datos no estructurados, como texto e imágenes, y automatizan la extracción de características, eliminando parte de la dependencia de expertos humanos. Esta característica del \textit{deep learning} será la que permita procesar las imágenes de este proyecto y realizar predicciones sobre otras.


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{Imagenes/IA/IAMLDL.png}
    \caption{ La inteligencia artificial, el aprendizaje automático y el aprendizaje profundo son distintos conceptos dentro de las ciencias de la computación }
    \label{fig:IAMLDL}
\end{figure}
\vspace{0.4cm}




\section{ Redes neuronales }


Como se explica en la Sección \ref{HistoriaIA}, las redes neuronales han permitido conseguir grandes logros a la inteligencia artificial, provocando que se sitúen como la el algoritmo más utilizado y con un mayor respaldo e investigación. A continuación, se abordan los principales conceptos que definen a una red neuronal, a una red neuronal profunda y a una red neuronal convolucional.

\subsection{ Neurona, funciones de activación, descenso del gradiente y backpropagation } \label{DescensoDelGradiente}

Durante mediados del siglo XX, existía discusión entre los principales investigadores en el campo de la inteligencia artificial sobre si la forma de crear máquinas inteligentes sería con una estrategia \textbf{bottom-up} o \textbf{top-down}. Las primeras estrategias se basarían en recrear la biología humana de forma que a partir de neuronas o pequeñas unidades la máquina pudiera aprender, la segunda trataba de simular directamente el alto nivel de decisiones en que se basa el comportamiento humano.

Las redes neuronales, como su propio nombre indica, forman parte de los algoritmos \textit{bottom-up}, puesto que están formadas por pequeñas unidades o \textbf{neuronas} que, al igual que en el sistema nervioso humano, se juntan para formar redes que definan su comportamiento.


La neurona artificial o unidad, que se puede ver en la Figura \ref{fig:neurona}, es el bloque de construcción básico de todas las redes neuronales artificiales. Una unidad toma como entrada un vector $x$ de $D$ números reales y produce como salida un número real (un escalar) $y$. La unidad está parametrizada por un vector de pesos $w$ de tamaño $D$ y un término de sesgo (un escalar) $b$ \cite{EzequielFundamentals}. De esta forma, se aplica una función $f$ a una entrada, resumiéndose este cálculo en la Ecuación \ref{eq:neurona}.

\begin{equation}
    y = f \left( b+\sum_{i=1}^{D} w_i x_{i} \right) = f \left( b + w^T x \right)
    \label{eq:neurona}
\end{equation}


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.7]{Imagenes/IA/ArtificialNeuron2.png}
    \caption{ Diagrama de la arquitectura de una neurona artificial comparada con una neurona biológica }
    \label{fig:neurona}
\end{figure}
\vspace{0.4cm}

Las neuronas artificiales se juntan para formar \textbf{redes neuronales} artificiales, existiendo diversos tipos de redes según la forma en que estas neuronas forman conexiones. La forma más simple de conectar las neuronas es mediante el \textit{feed forward} o prealimentación, en las cuales las redes neuronales forman capas secuenciales, existiendo un único flujo de información de las capas más cerca a la entrada hacia la salida. En la Figura \ref{fig:RedNeuronal} se aprecia una red neuronal de este tipo, existiendo capas de entrada, de salida y capas intermedias, llamadas ocultas \cite{EzequielFundamentals}. 


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.15]{Imagenes/IA/RedesNeuronales.png}
    \caption{ Diagrama de la arquitectura de una red neuronal }
    \label{fig:RedNeuronal}
\end{figure}
\vspace{0.4cm}


De esta forma, las redes neuronales están formadas por neuronas que aplican diferentes funciones a una entrada, dando una salida. Es importante hacer ver en este punto, que la suma de funciones lineales es una función lineal, por lo que si la función que aplican las neuronas a la entrada es lineal, una red neuronal formada por un gran número de neuronas se podría resumir en una única neurona aplicando una función lineal a la entrada y dando una salida. Dado que el objetivo de una red neuronal es conseguir resultados no lineales para problemas complejos, la función que se aplique a la entrada debe ser una función no lineal, siendo estas las \textbf{funciones de activación}. En la Figura \ref{fig:FuncActiv} se aprecian las principales funciones de activación utilizadas: la sigmoide, RelU y la tangente \cite{ArticuloBackpropagationReglaCdena}.


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{Imagenes/IA/FuncionesActivacion.png}
    \caption{ Principales funciones de activación utilizadas junto a sus derivadas \cite{ArticuloBackpropagationReglaCdena}}
    \label{fig:FuncActiv}
\end{figure}
\vspace{0.4cm}


Al hacer una recopilación de lo visto hasta hora, las redes neuronales son algoritmos formados por unidades llamadas neuronas que aplican una función a la información de entrada, dando una salida. Estas neuronas tienen diferentes entradas y dan distinto peso o importancia a cada entrada, dando a su vez la red neuronal diferente peso a cada neurona. En este punto es donde se halla verdadero poder de una red neuronal, puesto que las redes neuronales ``aprenden'' al dar distintos pesos a las neuronas hasta llegar a la mejor solución. Para calcular qué pesos en cada neurona llevan a una mejor solución, se aplica un problema de optimización, buscando que modelo minimiza el error final, para lo que se utiliza el algoritmo del \textbf{descenso del gradiente}. En la Figura \ref{fig:DescGrad} se observa como para una función no lineal de dos parámetros, este algoritmo encuentra el mínimo en el error, siendo este el eje $y$. Es importante tener en cuenta que en una red neuronal las funciones están formadas por multitud de parámetros y no solo dos.

\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Imagenes/IA/DescensoDelGradiente2.png}
    \caption{El algoritmo del descenso del gradiente calcula los parámetros para los que el modelo tiene un error mínimo \cite{VideoCSVRedeNeuronales2}}
    \label{fig:DescGrad}
\end{figure}
\vspace{0.4cm}


En este punto, el cálculo del modelo que obtiene un error mínimo se complica, puesto que se hace necesario conocer como influye cada parámetro de cada neurona al error final, es decir, el aplicar el algoritmo del descenso del gradiente implica el cálculo de un conjunto muy alto de derivadas parciales altamente no triviales. Esta gran necesidad matemática fue la que provoco el primer invierno de la IA, hasta que el 1986 se introdujo el algoritmo del \textit{\textbf{backpropagation}} o propagación hacia atrás \cite{LibroFinInvierno}. Este algoritmo simplifica el cálculo de las derivadas parciales para el peso de una neurona concreta, puesto que toma en el cálculo la capa próxima como la última capa, sin necesidad de tomar en cuenta todas las neuronas que forman cada posible camino \cite{VideoCSVBackPropagation}. 

El proceso comienza calculando el error entre la salida final de la red neuronal y el valor deseado. Este error se propaga hacia atrás capa por capa, calculando las contribuciones de cada neurona y sus pesos. De esta forma, el cálculo de las derivadas asociadas al descenso del gradiente se simplifica y, gracias a la regla de la cadena, las derivadas solo tiene una capa de cálculo \cite{ArticuloBackpropagationReglaCdena}. 



\vspace{0.4cm}
\begin{figure}[hbpt]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=0.55]{Imagenes/IA/Backpropagation.png}
                    \caption{Cálculo del gradiente sin \textit{backpropagation}}
                    \label{fig:Backpropagation1}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=0.55]{Imagenes/IA/Backpropagation2.png}
                    \caption{Cálculo del gradiente con \textit{backpropagation}}
                    \label{fig:Backpropagation2}
	 	\end{subfigure}
   \caption{Antes de existir el algoritmo del \textit{backpropagation}, las pérdidas eran calculadas mediante algoritmos
   de ``fuerza bruta'' llamados perturbación aleatoria \cite{VideoCSVBackPropagation} }
    \label{fig:Backpropagation}
\end{figure}



Gracias a estos conceptos, las redes neuronales actuales están formadas por cientos o miles de capas, por lo que se les considera con el término de \textbf{aprendizaje profundo} o \textit{\textbf{deep learning}}.



\subsection{ Redes neuronales convolucionales (CNN) }\label{RedesNeuronalesCNN}


Dado que en este proyecto se centra en la clasificación de imágenes, se utilizaran las \textbf{redes neuronales convolucionales}, ya que son las principalmente utilizadas para la clasificación de objetos en imágenes naturales \cite{EzequielCNN}.


\subsubsection{Capas de convolución}


Mientras que una red neuronal simple o \textit{vanilla} toma los píxeles de una imagen como valores independientes, una red neuronal convolucional saca partido de la posición espacial de los píxeles en la imagen. Esta pequeña diferencia supone un gran avance en la visión por ordenador, puesto que es de gran relevancia la posición espacial de los píxeles para comprender lo que la máquina está viendo \cite{VideoCSVCNN}. Para poder captar esta información, las redes neuronales convolucionales cuentan con \textbf{capas convolucionales}, donde se realiza una \textbf{operación de convolución}. 

Dado un vector $A$ de entrada y un kernel $K$, una operación de convolución de una dimensión devuelve un vector $B$ de la forma en que se aprecia en la ecuación \ref{eq:Convolucion}, siendo la operación el símbolo $*$. En la ecuación \ref{eq:Convolucion2D} se aprecia la operación en  dos dimensiones. En la Figura \ref{fig:Convolucion} se observa el ejemplo de una operación de convolución en dos dimensiones.



\begin{equation}
    B (t) = (A*K)(t) = \sum_{i} A (t+i) K(i)
    \label{eq:Convolucion}
\end{equation}


\begin{equation}
    B (t) = (A*K)(t) = \sum_{i} A (t+i) K(i)
    \label{eq:Convolucion2D}
\end{equation}


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Imagenes/IA/Convolucion2D.png}
    \caption{Ejemplo de operación de convolución 2D \cite{EzequielCNN}}
    \label{fig:Convolucion}
\end{figure}
\vspace{0.4cm}



La entrada y la salida de la convolución tienen un tamaño parecido, mientras que el tamaño del kernel es bastante menor. El kernel recorre la entrada, dando como resultado valores grandes y positivos para secciones de entrada similares al propio kernel.

En el caso de imágenes del mundo real, estas están formadas por tres capas (rojo, verde y azul), por lo que la entrada tendrá un tamaño: largo (ej. 5 píxeles) x ancho (ej. 5 píxeles) x 3 (RGB), de forma que un kernel de 3x3x1 recorrerá la entrada, como aparece en la Figura \ref{fig:CalculosConvolucion}. Como se comprueba en estos ejemplos de convolución, las matrices se completan con ceros con la técnica de rellenado o \textit{valid padding} \cite{BuenArticuloConvolucion}.

Los elementos del kernel $K$ son los pesos de aprendizaje de las neuronas de la capa de convolución, seguidos de una matriz de activación. Cada canal aporta diferentes perspectivas de la imagen, dando importancia a diferentes cualidades significativas de la imagen.


\vspace{0.4cm}
\begin{figure}[hbpt]
	 	\centering
	 	\begin{subfigure}[b]{0.2\linewidth}
	 	\centering
	 		\includegraphics[scale=0.3]{Imagenes/IA/Convolucion3D.png}
                    \caption{ Diagrama convolución 3D  }
                    \label{fig:Convolucion3D}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.7\linewidth}
	 	\centering
	 		\includegraphics[scale=0.6]{Imagenes/IA/Convolucion3DCalculos.png}
                    \caption{ Ejemplo cálculo convolución 3D }
                    \label{fig:Convolucion3DCalculos}
	 	\end{subfigure}
	 	\caption{ Las imágenes naturales están formadas por capas de colores y tamaño en píxeles  \cite{BuenArticuloConvolucion} } 
	 	\label{fig:CalculosConvolucion}
\end{figure}



\subsubsection{Capas \textit{pooling}}

La operación \textit{pooling} o sondeo se aplica a una sección de la entrada y produce un número real que represente esa sección. De esta forma, la operación reduce el tamaño de la entrada, disminuyendo la necesidad computacional, dado que las capas convolucionales mantienen el tamaño de la entrada, y extraen información de las características dominantes de la entrada. Esta operación es la realizada por las capas \textit{pooling}, situadas tras las convolucionales.

Existen distintos tipos de \textit{pooling}, siendo los principales \textit{max pooling} y \textit{average pooling}, representados con un ejemplo en la Figura \ref{fig:Pooling}. Normalmente, el método utilizado es \textit{max pooling}, que además cumple la función de aislante ante el ruido \cite{BuenArticuloConvolucion}. 

\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{Imagenes/IA/ConvCapaPooling.png}
    \caption{Ejemplo de operación de \textit{max} y \textit{average pooling} \cite{BuenArticuloConvolucion}}
    \label{fig:Pooling}
\end{figure}
\vspace{0.4cm}



\subsubsection{Capas \textit{fully connected}}


Tras las capas convolucionales y las \textit{pooling}, la entrada se aplana y se alimenta a las capas \textbf{\textit{fully connected}} o \textbf{completamente conectadas}. En estas capas, todas las neuronas están conectadas entre sí, lo que significa que cada neurona en una capa está conectada a todas las neuronas de la capa anterior. Esta estructura de conectividad completa permite que las capas totalmente conectadas realicen combinaciones lineales de las características de entrada y aprendan representaciones más complejas y abstractas.

Cada neurona en una capa totalmente conectada realiza una combinación lineal de las entradas que recibe, multiplicando cada entrada por su correspondiente peso y sumándolos junto con un término de sesgo (bias). Luego, se aplica una función de activación no lineal a la salida de la combinación lineal. Esta función de activación introduce no linealidad en la red y permite aprender relaciones no lineales entre las características.


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{Imagenes/IA/Fully connected.png}
    \caption{ Tras las capas convolucionales y \textit{pooling} las capas \textit{fully connected} obtienen características más complejas y abstractas \cite{BuenArticuloConvolucion}}
    \label{fig:FullyConnected}
\end{figure}



\subsubsection{Capas \textit{softmax}}


Cuando las clases de clasificación del problema son más de dos, surge la necesidad de una capa que se ocupe de esta clasificación, siendo estas las \textbf{capas \textit{softmax}}. 

Como se describe en la ecuación \ref{eq:Softmax}, la función de activación de las capas \textit{softmax} es una generalización de la función logística. Esta función toma un vector $x$, cuyo tamaño es el número de clases $C$, diferente para cada unidad $i$, donde $i$ es el índice de la clase.



\begin{equation}
    f_{i} = \frac{exp(x_{i})}{\sum^{C}_{j=1} exp(x_{j})}
    \label{eq:Softmax}
\end{equation}


En la Figura \ref{fig:Softmax} se demuestra un ejemplo de como las capas \textit{softmax} generan probabilidades para cada clase, siendo la suma de estas 1.

\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{Imagenes/IA/softmaxfunction.png}
    \caption{ Diagrama resumen de la función de las capas \textit{softmax} \cite{BuenArticuloSoftMax}}
    \label{fig:Softmax}
\end{figure}





\subsubsection{Red neuronal convolucional completa}


Todas las capas que forman parte de una red neuronal convolucional se unen de manera secuencial, dando lugar a este tipo de red neuronal. Se puede observar en la Figura \ref{fig:CNN} un diagrama simplificado de una de estas redes, la cual es capaz de aprender las características de un tipo de clase y clasificar en esta todas las imágenes dentro de esta.

\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{Imagenes/IA/CNN.jpg}
    \caption{ Diagrama resumen de una red CNN completa \cite{BuenArticuloConvolucion}}
    \label{fig:CNN}
\end{figure}
\vspace{0.4cm}




\subsection{Conclusiones}


La inteligencia artificial y su desarrollo son, sin duda, uno de los mayores hitos de la informática, la ciencia y la ingeniería en el comienzo del siglo XXI y en la historia de la humanidad. Su importancia es, además, que no solo es una tecnología aplicable a campos como la ciencia o la ingeniería, sino que ya es comprobable su uso en la vida diaria de cualquier persona.

En el punto en el que se encuentra la redacción de este texto, es aún pronto para conocer los posibles usos e influencia que llegaran a tener la inteligencia artificial y las redes neuronales. Sin embargo, la capacidad de usar el poder de las redes neuronales depende en gran parte de la capacidad innovadora para aplicarla a un problema que no tenga otra solución por parte de los ingenieros. 

Este es el caso de este proyecto, en el que los problemas asociados a la impresión con filamento fundido, vistos en el Apartado \ref{ManufAditiv}, se pueden afrontar utilizando una inteligencia artificial y, en concreto, una red neuronal convolucional para el reconocimiento de las imágenes tras la impresión.



















\chapter{Conjunto de datos (Datasets)}\label{section:Dataset}


Para entrenar la red neuronal convolucional que detecte las distintas zonas en una pieza de impresión por manufactura aditiva, se ha utilizado un conjunto de imágenes desarrollado por el Instituto de Tecnología Stevens \cite{AMDataset}.


La obtención de los datos se llevó a cabo con un perfilador láser 2D de alta velocidad LJ-V7060 de Keyence equipado en lo alto de una máquina de impresión de fabricación por filamento fundido. Después de cada impresión de capa, el escáner láser obtiene la nube de puntos en 3D de la superficie superior. A continuación, la nube de puntos 3D se preprocesa para lograr la superficie objetivo de la pieza. Luego, la superficie superior se segmenta a partir de la nube de puntos y se convierte en una imagen de profundidad 2D, que serán las utilizadas para la clasificación. En la Figura \ref{fig:DatasetSetup} se presenta los recursos y su disposición para la obtención de las imágenes.



\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{Imagenes/Dataset/DatasetSetting.png}
    \caption{ Configuración de los recursos para la obtención del conjunto de imágenes \cite{AMDataset} }
    \label{fig:DatasetSetup}
\end{figure}
\vspace{0.4cm}



\section{Descripción de los datos}


Para la obtención del conjunto de datos se han impreso una serie de piezas y obtenido su representación 2D de profundidad, como se ve en el ejemplo de la Figura \ref{fig:DatasetPieza}. Los colores verdes servirán para representar zonas de la pieza cuyo finalizado se puede considerar correcto. Las zonas de color azules no han tenido el suficiente material necesario, mientras que las zonas de color naranja, marrón y rojo cuentan con en exceso. Es importante también ver que la pieza puede tener, como en este caso, zonas blancas vacías que también se deben reconocer.



\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{Imagenes/Dataset/DatasetProfundidadPieza.png}
    \caption{ Imagen 2D para una pieza imprimida \cite{AMDataset} }
    \label{fig:DatasetPieza}
\end{figure}
\vspace{0.4cm}




En la Figura \ref{fig:Dataset4Imagenes} se observan las imágenes presentes en el \textit{dataset}. Cada conjunto de datos del mapa de altura de una pieza ha sido dividido en una cuadrícula de 10x10 (100 segmentos en total), y cada segmento ha sido categorizado por profesionales en cuatro categorías: (a) Over: Situación de sobreimpresión, (b) Under: Situación de subimpresión, (c) OK: Situación de impresión normal, (d) Empty: Vacío.

Hay 434 escaneos de piezas, por lo que la división en 100 segmentos hace que el \textit{dataset} esté formado por 43400 imágenes. En la Tabla \ref{tab:Dataset} se encuentra resumida la clasificación de las imágenes.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[HTML]{FFFFFF} 
{\color[HTML]{000000} Categoría} & Nombre & Total  \\ \hline
1                                & Over   & 12.268 \\ \hline
2                                & OK     & 13.228 \\ \hline
3                                & Under  & 14.726 \\ \hline
4                                & Empty  & 3.179  \\ \hline
                                 &        & 43.401 \\ \hline
\end{tabular}
\caption{Resumen del conjunto de datos}
\label{tab:Dataset}
\end{table}


\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	    \centering
	 		\includegraphics[scale=1.3]{Imagenes/Dataset/Over.png}
                    \caption{ Over } 
	 	    \label{fig:Dataset4Over}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	    \centering
	 		\includegraphics[scale=1.3]{Imagenes/Dataset/Under.png}
                    \caption{ Under } 
	 	    \label{fig:Dataset4Under}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	    \centering
	 		\includegraphics[scale=1.3]{Imagenes/Dataset/OK.png}
                    \caption{ OK } 
	 	    \label{fig:Dataset4OK}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	    \centering
	 		\includegraphics[scale=1.3]{Imagenes/Dataset/Empty.png}
                    \caption{ Empty } 
	 	    \label{fig:Dataset4Empty}
	 	\end{subfigure}
	 	\caption{ Ejemplos de las cuatro clases del conjunto de datos \cite{AMDataset}} 
	 	\label{fig:Dataset4Imagenes}
\end{figure}








































\chapter{Metodología}


\section{Python}

Python se ha convertido en uno de los lenguajes de programación más utilizados a nivel mundial gracias, principalmente, a su sintaxis, al ser fácil de leer y asemejarse más al lenguaje natural, además de ser de código abierto, portable entre plataformas, y posee una amplia librería con una gran comunidad alrededor \cite{ArticuloPythonGithub}.

Estas ventajas han provocado que Python sea el principal lenguaje de programación para tareas complejas como son el \textit{big data}, desarrollo web y de software e inteligencia artificial. El propio creador de este lenguaje, Guido van Rossum, explica como en otros lenguajes la productividad se ve afectada si se emplea demasiado tiempo en la realización del código, dejándose de usar este tiempo para la comparación de soluciones \cite{LexFridmanGuidoVanRossum}.

De esta forma, el lenguaje Python, permitirá una sintaxis sencilla con la cual comparar resultados de diferentes entrenamientos.



\section{PyTorch}

PyTorch es una biblioteca para programas en Python que facilita la construcción de proyectos de aprendizaje profundo. Su énfasis ese encuentra en la flexibilidad, permitiendo que los modelos de aprendizaje profundo se expresen en Python idiomático. Esta accesibilidad y facilidad de uso encontraron primeros adoptantes en la comunidad de investigación, y en los años transcurridos desde su primer lanzamiento, se ha convertido en una de las herramientas de aprendizaje profundo más prominentes en una amplia gama de aplicaciones.

De la misma forma en que Python supone una introducción a la programación para personas inexpertas en este campo, debido a su simple sintaxis y legibilidad, PyTorch permite un fácil aprendizaje al aprendizaje profundo y el entrenamiento de
modelos para nuevos estudiantes de este campo (como el propio autor de este trabajo). \cite{DeepLearningPytorchBook}

\subsection{Tensores}

Los números de punto flotante son la forma en que una red maneja la información, por lo que se necesita una manera de codificar los datos del mundo real que se quieren procesar en algo digerible por una red y luego decodificar la salida de nuevo en algo que una persona pueda entender y utilizar. 


 Para este fin, PyTorch introduce una estructura de datos fundamental \cite{DeepLearningPytorchBook}: el tensor. Este concepto de tensor no es el mismo que se utiliza en campos como matemáticas, física o ingeniería, donde el término tensor viene junto con la noción de espacios, sistemas de referencia y transformaciones entre ellos. En el contexto del aprendizaje profundo, los tensores se refieren a la generalización de vectores y matrices a un número arbitrario de dimensiones, como se puede apreciar en la Figura \ref{fig:PyTorchTensor}. 


PyTorch no es la única biblioteca que se ocupa de matrices multidimensionales. NumPy es de lejos la biblioteca de matrices multidimensionales más popular, hasta el punto de que ahora se ha convertido en el lenguaje oficial de la ciencia de datos. PyTorch cuenta con una interoperabilidad perfecta con NumPy, lo que conlleva una integración de primera clase con el resto de las bibliotecas científicas en Python, como SciPy, Scikit-learn, y Pandas.
Los tensores de PyTorch tienen algunas propiedades superiores a las matrices de NumPy, como la capacidad de realizar operaciones muy rápidas en unidades de procesamiento gráfico (GPU), distribuir operaciones en múltiples dispositivos o máquinas y realizar un seguimiento del grafo de cálculos que los creó. Estas son características importantes cuando se implementa una biblioteca moderna de aprendizaje profundo.


\vspace{0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{Imagenes/TeoriaResultados/TensorPyTorch.png}
    \caption{ Boceto para el concepto de tensor en PyTorch \cite{DeepLearningPytorchBook} }
    \label{fig:PyTorchTensor}
\end{figure}
\vspace{0.4cm}



\section{ \text{Scikit-learn} }

Otras de las principales librerías de PyTorch que se utilizarán en este proyecto es \text{Scikit-learn}. Scikit-learn es una biblioteca para aprendizaje automático de código abierto para Python. Destaca por ser de gran ayuda en cuanto al procesamiento, la regresión, la clasificación y la selección de modelos. Se corresponde con una de las librerías más populares de Python, ya que contiene gran variedad de algoritmos de aprendizaje automático y herramientas de procesamiento y análisis de datos. Está diseñada para interoperar con las bibliotecas numéricas y científicas NumPy y SciPy. En este proyecto, se utilizará esta librería para la representación de datos, puesto que dispone de una sencilla interfaz para representar las curvas ROC y matrices de confusión \cite{ScikitTokio}.

\section{Google Collab}

El código en el que se basa este proyecto, se ha desarrollado a través de la herramienta gratuita de Google, Google Collab, la cual es un producto de Google Research. Este recurso permite a cualquier usuario escribir y ejecutar códigos de Python en el navegador, sin necesidad de instalar ningún software en su máquina. Es especialmente adecuado para tareas de aprendizaje automático, análisis de datos y educación. 

Desde un punto de vista más técnico, Colab es un servicio de cuaderno alojado de Jupyter que no requiere configuración y que ofrece acceso sin coste adicional a recursos informáticos, como GPUs. El uso de esta herramienta permite la ejecución de código con coste computacional, como el de este proyecto, en tiempos reducidos en comparación con hacerlo en CPUs personales \cite{GoogleColab}.

\begin{figure}[hbpt]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=0.5]{Imagenes/TeoriaResultados/Python.png}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=0.3]{Imagenes/TeoriaResultados/PyTorch.png}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=0.5]{Imagenes/TeoriaResultados/googlecollab.png}
	 	\end{subfigure}
	 	\caption{ Python, PyTorch y Google Collab son los recursos principales para el desarrollo de este proyecto } 
	 	\label{fig:PythonPytorchCollab}
\end{figure}




\section{ \textit{Transfer learning} o aprendizaje transferido}\label{section:ModelosPreentrenados}

\textit{Transfer learning}, o Aprendizaje por Transferencia, es una técnica de \textit{Deep Learning} en la que se utiliza un modelo previamente entrenado para una tarea como punto de partida para entrenar otro modelo que realiza una tarea similar. Esta técnica facilita y reduce el tiempo utilizado en entrenamiento y se utiliza comúnmente en aplicaciones de reconocimiento de imágenes, detección de objetos o reconocimiento de voz, entre otras \cite{TransferLearningMatlab}.

Las redes convolucionales son especialmente susceptibles a este tipo de técnicas, ya que las capas más bajas son independientes de la clasificación final, centrándose en diferenciar colores, esquinas, patrones o texturas. Son las últimas capas en una red neuronal las que se ocupan de clasificar y elegir el resultado de una imagen, especificando más en el problema del que se ocupa. Por ejemplo, si se trata de un programa que decide si una imagen es un perro o un gato, las últimas capas se empezarán a fijar en las diferencias distintivas entre estos, como la cola, las orejas, o el hocico.

Existen diferentes estrategias basadas en el concepto de \textbf{transfer learning}. En este proyecto, se ha utilizado el \textit{Fine tuning}, en el cual se mantienen la mayoría de pesos neuronales del modelo original, únicamente variando las capas finales necesarias para la nueva clasificación \cite{EzequielFeatureExtraction}


\vspace{0.8cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Imagenes/IA/TransferLearning.jpg}
    \caption{ El \textit{transfer learning} o aprendizaje transferido permite utilizar los conocimientos ya adquiridos por otro modelo }
    \label{fig:TransferLearning}
\end{figure}
\vspace{0.8cm}



\subsection{Principales modelos preentrenados}

Se utilizarán seis de los principales modelos preentrenados que ofrece la librería PyTorch: AlexNet, VGG, ResNet, Inception, DenseNet, SqueezeNet. Todos ellos han sido entrenados con el conjunto de datos Imagenet, el cual está formado por millones de imágenes divididas en 1000 clases, por lo que se modificará el código para cambiar la clasificación a las 4 clases presentes en el conjunto de datos de este proyecto \cite{PyTorchFinTuningModels}.


\newpage
\subsubsection{AlexNet}

Esta red es conocida por ganar el Desafío de Reconocimiento Visual de Gran Escala ImageNet (ILSVRC)-2010 e ILSVRC 2012, con los mejores resultados reportados hasta la fecha, lo que impulsó el interés por el aprendizaje profundo para resolver este tipo de problemas \cite{AlexNetPaper}. En esta red, los autores utilizaron Unidades Lineales Rectificadas (ReLUs) como funciones de activación.

\vspace{1.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.85]{Imagenes/TeoriaResultados/Alexnet2.png}
    \caption{Arquitectura de la red Alexnet \cite{AlexNetPaper}}
    \label{fig:Alexnet}
\end{figure}
\vspace{0.4cm}

\newpage
\subsubsection{VGG}

El grupo de geometría visual (VGG) de la Universidad de Oxford propuso redes CNN con 16 y 19 capas, conocidas popularmente como arquitecturas VGG-16 y VGG-19 \cite{VGGPaper}. El modelo VGG utiliza filtros convolucionales pequeños de 3x3 con \textit{stride} de 1 píxel (por comparación, Alexnet tiene 11x11 con un \textit{stride} de 4 píxeles), en lugar de un solo filtro grande, lo que mejora la función de decisión y acelera la convergencia del modelo. Además, el tamaño reducido del filtro reduce el sobreajuste del modelo durante el entrenamiento y permite capturar características espaciales de la imagen. La consistencia en el uso de filtros de 3x3 hace que el modelo sea fácil de manejar. 

Se utilizará un modelo VGG11, el más pequeño de la familia disponible en PyTorch, siendo los otros de 13, 16 y 19 capas.

\vspace{1.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.9]{Imagenes/TeoriaResultados/VGG.png}
    \caption{Arquitectura de la red VGG16 \cite{EzequielCNN}}
    \label{fig:VGG}
\end{figure}
\vspace{0.4cm}

\newpage
\subsubsection{ResNet}

El aumento de la profundidad de las redes, es decir, la adición de más capas a la red, como en VGG-16 y VGG-19, demostró que las redes podían aprender bien. Sin embargo, también expuso uno de los problemas importantes en el entrenamiento de redes más profundas: la degradación de la precisión del entrenamiento. Para superar este problema de degradación, se introdujo el marco de aprendizaje residual profundo \cite{ResNetPaper}. Las redes residuales (ResNets) están inspiradas en las redes VGG, pero tienen menores complejidades.

Para este proyecto, se ha utilizado Resnet18, una red neuronal convolucional con 18 capas de profundidad, aunque existen otras mayores; Resnet34, Resnet50, Resnet101 y Resnet152.

\vspace{1.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.1]{Imagenes/TeoriaResultados/Resnet.png}
    \caption{Arquitectura de la red Resnet \cite{ResNetArchitectureImage} }
    \label{fig:Resnet}
\end{figure}
\vspace{0.4cm}


\newpage
\subsubsection{Inception}

Inception v3 fue descrita por primera vez en el artículo \textit{Rethinking the inception architecture for computer vision} \cite{InceptionPaper}. Esta red es única porque tiene dos capas de salida durante el entrenamiento. La segunda salida se conoce como una salida auxiliar y se encuentra en la parte de AuxLogits de la red. La salida principal es una capa lineal al final de la red. Es importante destacar que, durante las pruebas, solo consideramos la salida principal.

Se utilizará Inception-v3, la cual utiliza la idea de factorizar filtros espaciales más grandes en filtros más pequeños, y reemplazar un filtro convolucional espacial simétrico con múltiples filtros asimétricos. Inception-v3 tiene 42 capas y cuesta más del doble que GoogLeNet, pero menos que las redes VGG \cite{PaperComparesModelsPapers}.

\vspace{1.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.9]{Imagenes/TeoriaResultados/Inception.png}
    \caption{Arquitectura de la red Inception \cite{InceptionPaper}}
    \label{fig:Inception}
\end{figure}
\vspace{0.4cm}


\newpage
\subsubsection{DenseNet}

Las redes convolucionales tradicionales tienen L conexiones entre capas, mientras que DenseNet conecta cada capa con todas las demás capas en un enfoque de alimentación directa, lo que resulta en menos parámetros que las redes convolucionales tradicionales. Además, DenseNet combina características mediante concatenación, lo que permite un flujo mejorado de información y gradientes en toda la red, lo que ayuda a entrenar redes más profundas. DenseNet también tiene efectos de regularización en conjuntos de entrenamiento pequeños, lo que reduce el sobreajuste durante el entrenamiento. Esta red fue introducida con el artículo \textit{Densely Connected Convolutional Networks"} \cite{DenseNetPaper}.

PyTorch tiene 4 tipos de esta arquitectura, pero en este trabajo se utilizará DenseNet-121.


\vspace{1.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.9]{Imagenes/TeoriaResultados/densenet1.png}
    \caption{Arquitectura de la red Alexnet \cite{DenseNetPaper}}
    \label{fig:Densenet}
\end{figure}
\vspace{0.4cm}

\newpage
\subsubsection{SqueezeNet}

Mediante el artículo \textit{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size} \cite{SqueezeNetPaper} se pretendió crear una red que reemplazara los filtros de 3x3 por filtros de 1x1, disminuyera el número de canales de entrada a los filtros de 3x3 y  realizará el submuestreo (downsampling) tarde en la red, de modo que las capas de convolución tengan mapas de activación grandes. De esta forma se crea, como indica el propio título del artículo, una red con mayor precisión y menos tamaño.


\subsubsection{Comparación empírica de modelos preentrenados}

Para una idea visual de las características de cada modelo, se muestra en la Figura \ref{fig:ComparingPretrained} los resultados obtenidos para otro conjunto de datos publicados en un artículo \cite{PaperComparesModelsPapers}, en el que también se utilizan algunos de los utilizados en este trabajo. Aunque el comportamiento para este conjunto de datos no tiene por qué ajustarse a estos resultados, se puede obtener una primera impresión de las cualidades de cada uno.

\hspace{0.4cm}
\begin{figure}[hbpt]
	\centering
	 \includegraphics[scale=0.5]{Imagenes/TeoriaResultados/DiferentPretrainedModels.jpg}
	 \caption{ Comparación de los diferentes modelos para un otro problema y conjunto de datos \cite{PaperComparesModelsPapers}}
	 \label{fig:ComparingPretrained}
\end{figure}


\newpage
\section{Aumentación de datos}\label{AumentacionDatos}

La aumentación de datos, o \textit{data augmentation} en inglés, es una técnica comúnmente utilizada en el campo de la inteligencia artificial para mejorar el rendimiento de los modelos de aprendizaje automático. Esta técnica se basa en generar nuevas muestras de datos a partir de los datos originales, pero con pequeñas modificaciones.

Las modificaciones que se aplican pueden ser rotaciones, zooms, desplazamientos, reflejos, entre otros, como se puede ver en la Figura \ref{fig:AumentacionDatos}. Al generar estas nuevas muestras de datos, se aumenta el tamaño del conjunto de entrenamiento y se evita el sobreajuste en los modelos de aprendizaje automático, lo que significa que el modelo aprende a generalizar mejor para nuevos datos en lugar de memorizar los datos de entrenamiento \cite{EzequielCNN} \cite{DataAugmentation}.

\begin{figure}[hbpt]
	\centering
	 \includegraphics[scale=0.6]{Imagenes/TeoriaResultados/DataAugmentation.jpg}
	 \caption{ Visualización gráfica de algunos procesos utilizados para la aumentación de datos \cite{DataAugmentation} }
	 \label{fig:AumentacionDatos}
\end{figure}

La aumentación de datos se aplica comúnmente en el procesamiento de imágenes, pero también se puede utilizar en otros tipos de datos, como texto y audio. Esta técnica ha demostrado ser efectiva para mejorar el rendimiento de los modelos de aprendizaje automático en diversas tareas, como la clasificación de imágenes, detección de objetos y reconocimiento de voz.


Aunque en este problema la amplia variedad de la muestra hace que no sea necesario aumentar el tamaño del dataset, se empleará esta técnica para evaluar si se mejora el rendimiento de nuestro modelo.


\subsection{Transformaciones biblioteca PyTorch}


La propia biblioteca de aprendizaje profundo PyTorch cuenta con transformaciones aplicables a imágenes para la aumentación del conjunto de datos. Algunos ejemplos de estas transformaciones se encuentran en la Figura \ref{fig:AumentacionDatosEjPytorch}. Las transformaciones utilizadas serán las siguientes:

\begin{itemize}
    \item Resize: cambiar el tamaño de la imagen.
    \item ColorJitter: cambia el brillo, contraste, saturación y tono de una imagen.
    \item RandomRotation: se gira la imagen un número de grados.
    \item GaussianBlur: desenfoque gaussiano.
\end{itemize}



\begin{figure}[hbpt]
	\centering
	 \includegraphics[scale=0.4]{Imagenes/TeoriaResultados/PyTorchDataAug.png}
	 \caption{ Ejemplos de transformaciones a imágenes proporcionadas por la biblioteca PyTorch }
	 \label{fig:AumentacionDatosEjPytorch}
\end{figure}

\subsection{Transformaciones biblioteca imgaug}

Además de las transformaciones de la biblioteca PyTorch, se han utilizado las transformaciones de la biblioteca imgaug, la cual se utiliza únicamente para este tipo de operaciones, y cuenta con transformaciones algo más complejas. Además, las operaciones de esta biblioteca y las de PyTorch son compatibles y puede combinarse al utilizarse en las imágenes. Algunas de las transformaciones utilizadas serán:

\begin{itemize}
    \item Sometimes(GaussianBlur(sigma=())): provoca un desenfoque gaussiano en algunas imágenes.
    \item Sometimes(Dropout()): provoca que en algunas imágenes haya píxeles eliminados.
\end{itemize}

En la Figura \ref{fig:AumentacionDatosIAA} se aprecian algunas de las complejas transformaciones que se pueden realizar fácilmente con la biblioteca imgaug.


\begin{figure}[hbpt]
	\centering
	 \includegraphics[scale=0.4]{Imagenes/TeoriaResultados/IAADataAug.png}
	 \caption{ Ejemplos de transformaciones a imágenes proporcionadas por la biblioteca PyTorch }
	 \label{fig:AumentacionDatosIAA}
\end{figure}

\newpage
\section{Hiperparámetros}\label{Hiperparametrosteoria}


Los hiperparámetros son parámetros cuyo valor controla el proceso de aprendizaje de un modelo y determinan cómo de bien un algoritmo de aprendizaje profundo llega a aprender. Existen otros parámetros, como el peso de cada neurona, que se consideran simplemente parámetro, pero el prefijo ``híper'' implica que estos hiperparámetros están a alto nivel \cite{WebHiperparametros}.

Estos valores son externos al modelo, puesto que estos no varían durante el entrenamiento del modelo, sino que es el propio diseñador del algoritmo quien debe estudiar y elegir los más adecuados.

En definitiva, cualquier parámetro que defina un modelo y que no se pueda cambiar durante el entrenamiento se considera un hiperparámetro. Ejemplos de esto son la tasa de aprendizaje de la función de optimización, el valor de la función de optimización, el número de épocas, el tamaño de dataset, y muchos otros.

Para este proyecto, el número de épocas y el tamaño del dataset serán lo primero que se definirá tras varios experimentos. Más tarde, se decidirá la mejor función de optimización y tasa de aprendizaje.


\subsection{Función de optimización}

Durante la sección \ref{RedesNeuronalesCNN} del apartado sobre inteligencia artificial, se explicaron las bases conceptuales y matemáticas de un modelo de red neuronal. En este, se explicó el método del descenso del gradiente como algoritmo de optimización que se utiliza para calcular los pesos de las neuronas. Existen, sin embargo, otros algoritmos de optimización, aunque el descenso del gradiente sea el más utilizado. 


Los \textbf{optimizadores} son algoritmos que se utilizan para actualizar los parámetros (pesos y bias) de las neuronas del modelo. Su objetivo es encontrar aquellos para los que el modelo tiene una mejor respuesta. Si se vuelve a tomar el ejemplo del algoritmo del descenso del gradiente, este está definido según la ecuación \ref{eq:DescensoGradiente}, donde $x$ es un punto, $\alpha$ la tasa de aprendizaje y f la función a optimizar.


\begin{equation}
    x_{1} = x_{0} - \alpha \nabla f(x_{0})
    \label{eq:DescensoGradiente}
\end{equation}


Dentro de la optimización, las \textbf{funciones de pérdida} reflejan la diferencia entre la salida predicha por la red neuronal y el valor real de a salida. Existen diferentes técnicas para el cálculo de esta perdida, aunque en este proyecto tan solo se utilizará la función de \textbf{entropía cruzada} o \textit{cross entropy loss}. Esta función de pérdida está definida según lo establecido en la ecuación \ref{eq:CrossEntropyLoss}, donde $x$ es la entrada, $w$ es su peso, $y$ es el objetivo y $C$ es el número de clases.

\begin{equation}
    loss = -w_{y} \cdot log \frac{exp(x_{y})}{\sum_{C=1}^{C} exp(x_{C})}
    \label{eq:CrossEntropyLoss}
\end{equation}

Se probarán y compararán cuatro de los principales métodos de optimización, que se explican a continuación, siendo todos ellos funciones en PyTorch cuyos valores de entrada son los parámetros a optimizar y la tasa de aprendizaje.

\subsubsection{ Descenso del gradiente estocástico o \textit{Stochastic gradient descent} (SGD)}

Se trata de una versión sencilla de aplicación del descenso del gradiente. SGD solo estima el gradiente para la pérdida a partir de una pequeña submuestra de puntos de datos, que se eligen estocásticamente (al azar), lo que le permite ejecutarse mucho más rápido a través de las iteraciones. Teóricamente, la función de pérdida no se minimiza tan bien como con BGD (descenso de gradiente por lotes). Sin embargo, en la práctica, la aproximación cercana que se obtiene en SGD para los valores de los parámetros puede ser lo suficientemente buena en muchos casos. Además, los procesos estocásticos son una forma de regularización, por lo que las redes generalmente generalizan mejor.

\subsubsection{Algoritmo de Gradiente Adaptativo o \textit{Adaptative Gradient Algorithm} (Adagrad) }


Es una variación del algoritmo SGD, introducido en 2011 \cite{AdagradPaper} en la que se utilizan distintas tasas de aprendizaje para cada variable, teniendo en cuenta el gradiente acumulado en cada, haciendo que a aquellas que tienen un gradiente acumulado mayor se les aplica una tasa de aprendizaje inferior y viceversa.

Este algoritmo de optimización es uno de los más utilizados, aunque un posible problema con el que se encuentra en ocasiones es que la tasa de aprendizaje para una variable decrezca demasiado rápido debido a la acumulación de  valores altos del gradiente al comenzar el entrenamiento, pudiendo provocar que el modelo no sea capaz de aproximarse al mínimo en la dimensión. \cite{AdaGradArticulo}.


\subsubsection{Adadelta}


Este algoritmo se introduce en 2012 por Matthew Zeiler \cite{AdadeltaPaper}. El método Adadelta se adapta dinámicamente a lo largo del tiempo, utilizando solo información de primer orden y con un costo computacional mínimo más allá del descenso del gradiente estocástico estándar. El método no requiere ajustes manuales de una tasa de aprendizaje y es robusto frente al ruido en la información de gradiente, diferentes opciones de arquitectura de modelo, diversas modalidades de datos y selección de hiperparámetros.

Adadelta es una extensión más robusta de Adagrad que adapta las tasas de aprendizaje en función de una ventana móvil de actualizaciones de gradientes, en lugar de acumular todos los gradientes pasados. De esta manera, Adadelta continúa aprendiendo incluso cuando se han realizado muchas actualizaciones. En comparación con Adagrad, en la versión original de Adadelta no es necesario establecer una tasa de aprendizaje inicial. En esta versión, se puede establecer una tasa de aprendizaje inicial, al igual que en la mayoría de los otros.


\subsubsection{Estimación Adaptativa de Momentos o \textit{Adaptative movement estimation} (Adam) }


Se introdujo en 2014 \cite{AdamPaper} como un algoritmo para la optimización de funciones objetivo estocásticas basado en estimaciones adaptativas de momentos de orden inferior. El método es fácil de implementar, eficiente computacionalmente, requiere poca memoria, es invariante a la reescala diagonal de los gradientes y es adecuado para problemas que son grandes en términos de datos y/o parámetros. El método también es apropiado para objetivos no estacionarios y problemas con gradientes muy ruidosos y/o dispersos.

La intuición detrás de este algoritmo es similar a la de SGD (descenso de gradiente estocástico). La diferencia principal es que los solucionadores Adam son notificadores adaptativos. Adam también ajusta la tasa de aprendizaje basándose en la magnitud de los gradientes utilizando la propagación de la media cuadrática de las raíces (RMSProp). Esto sigue una lógica similar al uso de momentum y amortiguamiento para SGD. Esto lo hace robusto para el paisaje de optimización no convexo de las redes neuronales \cite{RandomAdam}.




\subsection{Tasa de aprendizaje}\label{VariacionLearningRate}

Si la optimización consiste en encontrar un mínimo para una función concreta, se puede imaginar como si se buscará el lugar de menor altitud para una cordillera. Al hacer esto, se utiliza el gradiente para calcular cuanto se está bajando en cada dirección y encontrar el mínimo. En este símil, la tasa de aprendizaje o \textit{learning rate} sería el tamaño de la zancada a la hora de desplazarse por la cordillera y buscar el mínimo, de forma que dependiendo de la función, un paso mayor o menor nos hará encontrarlo con mayor facilidad.

Este símil se aplica también a la función de la Figura \ref{fig:TasaAprendizaje}, donde una tasa de aprendizaje demasiado pequeña llegará al mínimo, pero lo hará demasiado lento, y una tasa muy grande no será capaz de encontrarlo.

\hspace{0.4cm}
\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.7]{Imagenes/TeoriaResultados/LearningRate.png}
	 \caption{ Una tasa de aprendizaje adecuada es fundamental para encontrar la solución óptima a cualquier problema }
	 \label{fig:TasaAprendizaje}
\end{figure}
\hspace{0.4cm}

Existen dos aspectos que optimizar con respecto a la tasa de aprendizaje: 
\begin{itemize}
    \item La tasa de aprendizaje inicial: marca cuál va a ser la velocidad global a la que se van a modificar los parámetros de la red. Sí, es demasiado pequeña, la red tarda demasiado en aprender. Si es demasiado grande, la red va saltando entre configuraciones muy alejadas entre sí, y no converge a una configuración óptima.
    \item La variación de la tasa de aprendizaje durante el entrenamiento: La tasa de aprendizaje se podría dejar constante (gamma=1), pero habitualmente esto no es lo ideal porque conforme se va avanzando en el entrenamiento se procura acercarse poco a poco a una configuración óptima. Ahora bien, la velocidad a la que decrece la tasa de aprendizaje también hay que determinarla experimentalmente. Si gamma es demasiado alto, la red tarda mucho en converger. Si gamma es demasiado bajo, es posible que la red se quede atascada en una configuración no óptima.
\end{itemize}


PyTorch cuanta con diversas funciones que permiten variar la tasa de aprendizaje mientras el modelo se entrena, de las cuales se utilizaran dos de las más usadas, que se exponen a continuación.


\subsubsection{StepLR}

Esta función se utiliza para reducir el valor de la tasa de aprendizaje cada cierto número de épocas. De esta forma, la función acepta dos parámetros principales:

\begin{itemize}
    \item Tamaño del paso: se indica el número $N$ de épocas que deben pasar para cada reducción de la tasa.
    \item $\gamma$: un factor multiplicativo por el cual la tasa de aprendizaje se reduce. Por ejemplo, si la tasa de aprendizaje es 1000 y gamma es 0.5, la nueva tasa de aprendizaje será 1000 x 0.5 = 500.
\end{itemize}


En la Figura \ref{fig:TasaAprendizajeStep} se representa un ejemplo de la función StepLR.

\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.8]{Imagenes/TeoriaResultados/sTEPlr.png}
	 \caption{ Ejemplo de función StepLR para reducción de la tasa de aprendizaje }
	 \label{fig:TasaAprendizajeStep}
\end{figure}



\subsubsection{ExpotentialLR}

Parecida a la anterior, esta función genera una bajada en la tasa de aprendizaje con el paso de las épocas, en este caso exponencialmente, como se representa en la Figura \ref{fig:TasaAprendExpo}. Esta función tan solo necesita un término $\gamma$ para reducir la tasa cada época, tal como indica la ecuación \ref{eq:TasaAprenExponentialLR}.

\begin{equation}
    Tasa \, de\, aprendizaje_{1} \,=\, Tasa\, de\, aprendizaje_{0} \left( 1 - \frac{\gamma}{100}  \right)^{n epoca}
    \label{eq:TasaAprenExponentialLR}
\end{equation}

\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.6]{Imagenes/TeoriaResultados/ExpotentialLR.png}
	 \caption{ Ejemplo de función ExpotentialLR para reducción de la tasa de aprendizaje }
	 \label{fig:TasaAprendExpo}
\end{figure}




\newpage
\section{Optimización de hiperparámetros}


Si bien en este proyecto se utilizará una optimización de los parámetros e hiperparámetros a partir de los resultados de los experimentos, existen otras técnicas para mejorar estos parámetros, muchas de ellas descubiertas en los últimos años y otras aún en desarrollo. Debido a la relevancia de estos conceptos en el campo de la inteligencia artificial, se ha considerado oportuno el explicar estas ideas, si bien no se utilizaran empíricamente, existe la posibilidad de hacerlo en líneas futuras.


La elección de valores en la arquitectura de una red neuronal con base en intuición y un poco de prueba y error no es lo más apropiado, si bien es el más usado \cite{OptimizationPaper}, ya que no ayuda a saber como de eficiente es realmente nuestro modelo y si existe alguna combinación de arquitectura que pueda ayudarnos a obtener resultados generalizados mejores \cite{OptimizacionArtBasico}. Algunos de los hiperparámetros más relevantes son: 

\begin{itemize}
    \item Número de capas: un número muy alto puede causar problemas como sobreajuste, mientras que un número bajo puede hacer que el modelo tenga sesgo alto y un potencial bajo.
    \item Número de unidades ocultas por capa: encontrar un equilibrio entre sesgo alto y varianza.
    \item Función de activación: Las opciones populares son ReLU, Sigmoid y Tanh.
    \item Optimizador: es el algoritmo utilizado por el modelo para actualizar los pesos de cada capa después de cada iteración. Las opciones populares son SGD, RMSProp y Adam.
    \item Tasa de aprendizaje: es responsable de la característica central de aprendizaje y debe elegirse de tal manera que no sea demasiado alta, lo que dificultaría la convergencia al mínimo, ni demasiado baja, lo que dificultaría acelerar el proceso de aprendizaje.
    \item Dropout: consiste en eliminar ciertas conexiones en cada iteración, de modo que las unidades ocultas no dependan demasiado de ninguna característica en particular.
    \item Regularización L1/L2: actúa como otro regularizador en el que se controlan los valores de peso muy altos para evitar que el modelo dependa de una única característica
\end{itemize}

Además, para redes neuronales convolucionales como la que se emplea en este proyecto existen otros como:

\begin{itemize}
    \item Tamaño del Kernel/Filtro: como se explicó en el apartado \ref{IA}, un kernel es una matriz de pesos con la que se aplica la convolución a la entrada. Si se considera que se necesitan muchos píxeles para que la red reconozca el objeto, se usan filtros grandes (como 11x11 o 9x9). Si lo que diferencia a los objetos son algunas características pequeñas y locales, se usan filtros pequeños (3x3 o 5x5).
    \item Padding: se utiliza generalmente para agregar columnas y filas de ceros para mantener las dimensiones espaciales constantes después de la convolución. Hacer esto puede mejorar el rendimiento, ya que se conserva la información en los bordes.
    \item Stride: es el número de píxeles que se saltan al atravesar la entrada horizontal y verticalmente durante la convolución después de cada multiplicación de elementos de los pesos de entrada con los del filtro. Se utiliza para disminuir considerablemente el tamaño de la imagen de entrada.
    \item Parámetros de Capas de Pooling: las capas de pooling también tienen los mismos parámetros que una capa de convolución. La opción más comúnmente utilizada es el Max-Pooling.
\end{itemize}


Algunos de estos parámetros ya se han expuesto en el apartado anterior, como la tasa de aprendizaje o la función de optimación, y se trataran de optimizar mediante experimentos en la sección de resultados, aunque otros como el número de capas o el \textit{dropout} no se trataran en este proyecto. La optimización de hiperparámetros o HPO (\textit{Hyper-parameter optimization}) es un proceso sistemático que ayuda a encontrar los valores de hiperparámetros que permitan que el modelo ofrezca los mejores resultados.



\vspace{0.4cm}
\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.35]{Imagenes/TeoriaResultados/TuningHiperParametros.png}
	 \caption{ La optimización de hiperparámetros es un proceso sistemático que permite encontrar los mejores valores de hiperparámetros para el modelo \cite{OptimizacionArtBasico}}
	 \label{fig:HPO}
\end{figure}
\vspace{0.4cm}



Dentro de estos procesos, los principales son \textbf{\textit{Grid search}}, \textbf{\textit{Ramdon search}} y \textbf{búsqueda por optimización bayesiana}.

\begin{itemize}
    \item \textit{Grid search}: es una de las técnicas de optimización más sencillas, la cual funciona por medio de la comparación: se toman dos valores de un conjunto pequeño de valores para dos o más parámetros, se evalúan todas las combinaciones posibles y con ellas se forma una cuadrícula de valores (traducción de \textit{grid search}) \cite{EnlaceGridSearch}. Los principales pasos del método se pueden resumir de la siguiente manera: 1. determinar un conjunto de valores de hiperparámetros, 2. combinar los valores seleccionados y crear un modelo para cada combinación, y 3. probar cada combinación utilizando una técnica de validación. La principal ventaja de la búsqueda en cuadrícula es la posibilidad de realizar las diferentes pruebas en paralelo.

    Una de sus principales limitaciones es que el método solo se puede utilizar en el caso de un espacio de hiperparámetros de baja dimensionalidad, es decir, 1-D, 2-D, etc. El método es lento para un mayor número de parámetros. Además, no se puede aplicar para la selección de modelos, ya que solo se puede utilizar para ajustar un solo modelo \cite{PaperComparandoOptimaz}.

    \item \textbf{\textit{Ramdon search}}: se introdujo en 2012 \cite{PaperRamdonSearch} como una alternativa a la clásica técnica \textbf{grid search}. En este caso, los hiperparámetros se seleccionan al azar, de forma independiente de otras elecciones. El método es simple de implementar y es adecuado para aprender funciones libres de gradiente. En comparación con \textit{grid search}, este método converge más rápido, al buscar de manera efectiva en un espacio de hiperparámetros más grande y menos prometedor.
    
    Algunas de sus limitaciones es que es un método que consume mucho tiempo, ya que la evaluación de la función se vuelve costosa. El método no tiene la capacidad de selección de modelos, ya que solo se utiliza con un solo modelo. En comparación con la optimización bayesiana, este método no aprovecha el conocimiento de un espacio de búsqueda que funciona bien.


    \item Búsqueda bayesiana: introducida en 2012 \cite{PaperBayesian}, esta técnica se diferencia de la búsqueda aleatoria o en cuadrícula, en que llevan un registro de los resultados de evaluación pasados que utilizan para formar un modelo probabilístico que relaciona los hiperparámetros con la probabilidad de obtener un puntaje en la función objetivo \cite{BayesianSearchMedium}.
    
    En resumen, el proceso de optimización bayesiano se resume en:
    
    1. Construir un modelo de probabilidad sustituto de la función objetivo.
    2. Encontrar los hiperparámetros que funcionen mejor en el modelo sustituto.
    3. Aplicar estos hiperparámetros a la verdadera función objetivo.
    4. Actualizar el modelo sustituto incorporando los nuevos resultados.
    5. Repetir los pasos 2-4 hasta alcanzar el número máximo de iteraciones o el tiempo establecido.

    
    A nivel general, los métodos de optimización bayesianos son eficientes porque eligen los próximos hiperparámetros de manera informada. La idea básica es invertir un poco más de tiempo en seleccionar los próximos hiperparámetros para realizar menos llamadas a la función objetivo. En la práctica, el tiempo invertido en seleccionar los próximos hiperparámetros es insignificante en comparación con el tiempo dedicado a la función objetivo. Al evaluar hiperparámetros que parecen más prometedores según los resultados pasados, los métodos bayesianos pueden encontrar mejores configuraciones de modelo que la búsqueda aleatoria en menos iteraciones.
    
    Los métodos basados en modelos bayesianos pueden encontrar mejores hiperparámetros en menos tiempo porque razonan acerca del mejor conjunto de hiperparámetros a evaluar según las pruebas pasadas.

    \item Existen otras técnicas para la optimización de hiperparámetros \cite{PaperGrand} \cite{PaperHyper}, aunque estas son las más utilizadas.

\end{itemize}



Para el uso de estas técnicas en Python, existen bibliotecas diseñadas para su implementación, como Optuna o Hyperopt \cite{HPOPytorchLibrerias}.
















\newpage
\section{Validación}


Entre los conceptos explicados hasta este punto, falta uno de los detalles fundamentales relacionados con el entrenamiento de modelos basados en redes neuronales. Si bien se han explicado diferentes técnicas y métodos para el entrenamiento de un conjunto de datos, no se han introducido los diferentes grupos en los que se divide un \textit{datase}: entrenamiento (\textit{training}), validación (\textit{validation}) y prueba (\textit{test}).

El grupo de entrenamiento es el más sencillo, puesto que tan solo se trata del conjunto de datos que se utiliza para que el modelo aprenda las características de cada imagen. Por otro lado, para comprobar el rendimiento del modelo, no podemos hacerlo con las mismas imágenes con las que se está entrenando, puesto que el modelo podría memorizar estas y, aunque ofreciera una alta precisión, no sería una precisión real. Es por esto que para hacer el \textit{feedback} o realimentación que nos indique como está funcionando nuestro modelo, se crea otro grupo con imágenes que no se utilicen durante el entrenamiento, llamado conjunto de validación.

Si bien con el conjunto de validación se pueden definir los parámetros que hagan que el modelo aprenda mejor, el tener buenos resultados ante la validación no asegurara que el modelo tenga un buen rendimiento ante otras imágenes del mundo real, que es para lo que un modelo debe servir. Es por esto que existe un grupo de prueba con el que se comprueba el modelo una vez ha sido configurado con los conjuntos de entrenamiento y validación, siendo este el conjunto de prueba, \textit{testing} o testeo.

En la Figura \ref{fig:DataTrainValTest} se resumen estos tres conjuntos y su función.

\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.8]{Imagenes/TeoriaResultados/TiposDatos.png}
	 \caption{ División del conjunto de datos para las diferentes etapas en la comprobación de un modelo }
	 \label{fig:DataTrainValTest}
\end{figure}



La manera más sencilla de crear estos conjuntos, es mediante una división al azar, eligiendo un porcentaje determinado para cada conjunto sobre el \textit{dataset}. Algunos porcentajes comúnmente usados a la hora de dividir el conjunto de datos en el contexto del aprendizaje profundo son del 80\% para el conjunto de entrenamiento, 10\% para la validación y otro 10\% para el testeo \cite{PorcentajesValidacion}. Al ser este un trabajo simple y de introducción a este campo, tan solo se han utilizado los conjuntos de entrenamiento y validación, siendo suficientes para el proceso de elección y optimización del modelo de reconocimiento de las imágenes. Si bien, sería oportuno que este trabajo continuará realizando pruebas con las imágenes no usadas para entrenamiento y validación.



\newpage

\subsection{Validación cruzada o \textit{cross validation}}\label{TiposValidacionCruzada}


Aunque se ha explicado que la forma de entrenar un modelo es con conjuntos de entrenamiento y validación fijos creados al azar a partir del \textit{dataset},  esto se trata de una manera básica de entrenar y validar un modelo, conocida como validación \textbf{\textit{hold out}} \cite{HoldOut}. Esta será la manera en la que se valide el modelo durante los primeros entrenamientos en este trabajo: para comparar el modelo preentrenado frente al creado desde cero, comparar los diferentes modelos preentrenados, la aumentación de datos y la optimización de hiperparámetros.


Sin embargo, existe un fallo en la utilización de un entrenamiento con conjunto de datos de entrenamiento, validación y prueba fijos. Si bien el modelo se está entrenando, validando y probando, lo está haciendo siempre con las mismas imágenes para cada conjunto, por lo que es posible que no termine de aprender todo lo que pueda sobre el \textit{dataset}. Es por esto que en aprendizaje profundo se utiliza la \textbf{validación cruzada} o \textit{cross validation}, la cual se centra en crear diferentes entrenamientos, variando los conjuntos de entrenamiento y validación, de forma que le modelo pueda aprender de todas las imágenes.


Existen diversas formas de aplicar la validación cruzada \cite{TiposValidacionCruzada}, algunas de ellas se muestran en la Figura \ref{fig:CVTipos}. Se trata de diferentes maneras de variar los conjuntos de entrenamiento y validación (aunque se muestre en la imagen como \textit{testing}, los conceptos de validación y prueba son a veces intercambiados. Estas imágenes se refieren a los conjuntos de datos para validar el modelo, es decir, conjunto de validación). 

La manera más básica de elegir el conjunto de datos con validación cruzada sería la validación repetida por submuestreo aleatorio (\ref{fig:CVRamdon}), donde simplemente se repite durante varias iteraciones la validación \textit{hold out}. El siguiente ejemplo es una de las formas más utilizadas de validación cruzada, la validación cruzada por \textit{k-folds} (\ref{fig:CVKFoldsTeoria}), donde se dividen los datos de forma aleatoria en k grupos de aproximadamente el mismo tamaño. Similar a esta, la validación cruzada por grupos \ref{fig:CVGrupos} se utiliza si el conjunto de datos está formado por diferentes grupos además de las clases, escogiéndose un conjunto de grupo para validación en cada iteración. Por último, otra forma de realizar la validación cruzada es mediante la validación cruzada creciente con series de tiempo (\ref{fig:CVTiempo}), donde se divide el dataset como para validación \textit{k-folds}, pero empezando por uno de entrenamiento y uno de validación e ir aumentando en uno los subconjuntos de entrenamiento, hasta llegar al total del \textit{dataset}. 

Además, en las Figuras \ref{fig:CVEstratificadoGrupos} y \ref{fig:CVEstratificadoRandom} se muestra otra técnica común en la validación cruzada, la estratificación de la validación cruzada, de forma que además de repetir el entrenamiento con diferentes validaciones, se asegura que en el caso de que el \textit{dataset} no este equilibrado, el número de datos de cada clase sea porcentual al número de muestras en la clase total.

\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/TeoriaResultados/CrossValidationKFold.png}
                    \caption{ \textit{k-folds} } 
	 	        \label{fig:CVKFoldsTeoria}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/TeoriaResultados/CrossValidationRandom.png}
                    \caption{ Aleatoria } 
	 	        \label{fig:CVRamdon}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/TeoriaResultados/CrossValidationGroups.png}
                    \caption{ Grupos } 
	 	        \label{fig:CVGrupos}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/TeoriaResultados/CrossValidationTime.png}
                    \caption{ Por tiempo } 
	 	        \label{fig:CVTiempo}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/TeoriaResultados/CrossValidationStratified.png}
                    \caption{ Estratificada por grupos } 
	 	        \label{fig:CVEstratificadoGrupos}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/TeoriaResultados/CrossValidationStratifiedRandom.png}
                    \caption{ Estratificada aleatoria } 
	 	        \label{fig:CVEstratificadoRandom}
	 	\end{subfigure}
	 	\caption{ Diferentes tipos de validación cruzada explicados \cite{ScklearnCV} } 
	 	\label{fig:CVTipos}
\end{figure}



De entre las posibles técnicas de validación cruzada, se utilizarán el \textit{k-fold}, aleatorio y temporal. Si bien se podrían haber utilizado más ejemplos, el conjunto de datos utilizado en  este trabajo no está formado por grupos y la estratificación no es necesaria dado que las clases están igualadas en cuantos a las muestras en cada una.



\section{Representación de la información}

La inteligencia artificial y el deep learning son utilizadas para tratar con grandes cantidades de datos, el conocido como \textit{Big Data}. En casos como el presente en este proyecto, la comparación de resultados se hace complicada con algunos métodos gráficos clásicos, Es por ello, que también se han utilizado otros métodos para la representación de resultados para este tipo de técnicas, utilizándose en este trabajo las dos más extendidas.


\subsection{Precisión y perdida}


Aunque estos términos son algo triviales y no requieren una extensa explicación, gran parte de las comparaciones entre modelos se harán según la \textbf{precisión, o \textit{accuracy}}, y la \textbf{pérdida, o \textit{loss}}, que estos modelos ofrezcan, por los que se aclararan a continuación.

La precisión es un método para medir el rendimiento de un modelo de clasificación que se expresa típicamente como un porcentaje. Este valor es el recuento de predicciones en las que el valor predicho es igual al valor real. La precisión a menudo se representa gráficamente y se monitorea durante la fase de entrenamiento, aunque el valor suele asociarse con la precisión general o final del modelo. El valor de la precisión durante validación tiene una mayor importancia a la hora de escoger un modelo, puesto que muestra el rendimiento de ese modelo para reconocer imágenes fuera de su conjunto de entrenamiento. Esto implica que el modelo tendrá también una mayor precisión clasificando nuevas imágenes.

Una función de pérdida tiene en cuenta las probabilidades o la incertidumbre de una predicción en función de cuánto varía la predicción respecto al valor real. Esto brinda una visión más detallada de cómo está funcionando el modelo. A diferencia de la precisión, la pérdida no es un porcentaje, es una suma de los errores cometidos para cada muestra en los conjuntos de entrenamiento o validación. Por esto, la precisión tiene una mayor facilidad para interpretarse frente a la pérdida.

En la Figura \ref{fig:EjAcc&Loss} se ha representado un ejemplo de la precisión y pérdidas para un modelo cualquiera.


\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.6]{Imagenes/TeoriaResultados/AccLoss.png}
	 \caption{ Gráfica para la precisión y pérdidas en entrenamiento y validación  }
	 \label{fig:EjAcc&Loss}
\end{figure}



\subsection{\textit{Overfitting}}

Si bien este concepto se podría haber explicado junto a las redes neuronales convolucionales, se ha decidido redactar en este apartado puesto que es un concepto fundamental a la hora de entender los resultados que un modelo ofrece.

\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.3]{Imagenes/TeoriaResultados/Overfitting.png}
	 \caption{ Overfitting explicado \cite{OverfittingImagen} }
	 \label{fig:Overfitting}
\end{figure}

El \textit{overfitting} o sobreajuste es un error que se produce cuando una función está demasiado alineada con un conjunto limitado de datos. El resultado es un modelo que es útil solo cuando se refiere a su conjunto de datos inicial y no a cualquier otro conjunto de datos. La intuición con respecto a por qué se da este error se puede explicar como que el modelo aprende ``de memoria'' los datos dentro de un conjunto en lugar de aprender las características que definen cada clase de datos. De esta forma, como se explica en la Figura \ref{fig:Overfitting} el modelo no servirá para lo que interesa: tener un buen rendimiento con un conjunto de datos nuevos \cite{Overfitting}.


El \textit{overfitting} se puede reconocer fácilmente si un modelo no está generando buenos datos de validación y se observa que la precisión en entrenamiento continua aumentando. Puede ocurrir que el modelo genere buenos datos de entrenamiento y validación hasta que llegue un punto donde el entrenamiento mejora y la validación no, como se observa en la Figura \ref{fig:Overfitting2}, ante lo cual la solución sería cortar el entrenamiento antes. Otras soluciones a la hora de entrenar modelos con \textit{overfitting} pueden ser la validación cruzada, la aumentación de datos o el \textit{pruning}, donde se eliminan algunos parámetros del modelo.

\begin{figure}[hbpt]
      \centering
	 \includegraphics[scale=0.7]{Imagenes/TeoriaResultados/Overfittin2.png}
	 \caption{ El \textit{overfitting} puede comenzar en un punto donde la validación y el entrenamiento no avanzan igual }
	 \label{fig:Overfitting2}
\end{figure}


\subsection{Matriz de confusión}\label{ConfMat}

 La representación de la  precisión o \textit{accuracy}. así como de la pérdida o \textit{loss}, del modelo en sus diferentes etapas permite evaluar fácilmente si el modelo cumple o no con nuestras necesidades. Sin embargo, estos parámetros tan solo aportan información sobre el resultado del modelo y no de lo que ocurre en su interior y cómo de bien está clasificando las diferentes clases. Ante esto, la matriz de confusión es la principal respuesta para entender el comportamiento interior del modelo.  
 
 Una matriz de confusión es una representación visual del rendimiento de un modelo de aprendizaje automático, mediante el resumen de los valores correctos y predichos de la clasificación. Los resultados se aprecian divididos según la clasificación real y la que ha predicho el modelo, pudiendo distinguir en qué tipo de datos ha fallado el modelo. 

Como se observa en la Figura \ref{fig:MatrizCofusionSimple}, un modelo de calcificación binaria cuyas dos salidas sean verdadero o positivo pueden tener cuatro tipos de resultados \cite{ConfusionMatixSimple}:

\begin{itemize}
    \item \textit{True positive} o verdaderos positivos (TP): el modelo supone verdadero y el valor es verdadero.
    \item \textit{False positive} o falsos positivos (FP): el modelo supone verdadero y el valor es negativo.
    \item \textit{True negative} o verdaderos negativos (TN): el modelo supone negativo y el valor es negativo.
    \item \textit{False negative} o falsos negativos (FN): el modelo supone negativo y el valor es verdadero.
\end{itemize}

\begin{figure}[hbpt]
	\centering
	 \includegraphics[scale=0.5]{Imagenes/TeoriaResultados/ConfusionMatrixSimple.png}
	 \caption{ Posibles salidas de una matriz de confusión binaria \cite{ConfusionMatixSimple} }
	 \label{fig:MatrizCofusionSimple}
\end{figure}

Esta gráfica aporta una información que se hizo evidente durante la pandemia de COVID-19 para la mayor parte de la población mundial, y es que algunos fallos por parte de un modelo tienen más importancia que otros. De igual forma que un falso positivo de COVID u otra enfermedad es preferible a uno falso negativo, un modelo de aprendizaje profundo que al reconocer imágenes de ecografías para detectar cáncer de mama deberá centrarse en no clasificar un resultado positivo como negativo.


El modelo utilizado en este proyecto no es un modelo binario, sino que clasifica las imágenes entre cuatro posibilidades, por lo que se usará un modelo basado en el de la Figura \ref{fig:MatrizCofusion}, el cual clasifica imágenes de diferentes tipos de flores. Al igual que en el ejemplo médico, la clasificación errónea de una clase tendrá una mayor importancia según la clase con la que es confundida. Es sencillo visualizar que un parte de la pieza donde sobra material, siendo confundida con una en la que falta, supondría un mayor problema, puesto que se estaría añadiendo material en un lugar donde ya existe demasiado.

\begin{figure}[hbpt]
	\centering
	 \includegraphics[scale=0.6]{Imagenes/TeoriaResultados/ConfusionMatrix.png}
	 \caption{ Posibles salidas de una matriz de confusión binaria \cite{ConfusionMatix} }
	 \label{fig:MatrizCofusion}
\end{figure}


\subsection{Curva ROC}


La curva ROC (\textit{receiver operating characteristic}), al igual que la matriz de confusión, aporta información sobre la clasificación interna que hace el modelo, con la diferencia de que la curva ROC permite representar esta información gráficamente. Esta curva se genera calculando la tasa de verdaderos positivos (TPR), expresada en la ecuación \ref{eq:TPR} contra la tasa de falsos positivos (FPR), expresada en la ecuación \ref{eq:FPR} \cite{ROCCurveTeorica}, para cada posible límite en el porcentaje de elección de una clase, que es un porcentaje entre 0 y 100 \% o entre 0 y 1, como aparece en la gráfica que se utilizará.


\begin{equation}
    TPR = \textit{Sensibility} (Sensibilidad) = \frac{TP}{TP+FN}
    \label{eq:TPR}
\end{equation}

\begin{equation}
    FPR = (1 - \textit{Specifity}(Especifidad)) = \frac{FP}{TN+FP}
    \label{eq:FPR}
\end{equation}

Por tanto, esta gráfica permite representar como de bien clasifica un modelo una clase frente a otra.

En la Figura \ref{fig:CurvaROC} se observa una gráfica con ejemplos para entender intuitivamente el concepto tras la curva ROC. Un modelo perfecto será aquel que para cualquier límite en el porcentaje de clasificación de una clase tiene una sensibilidad perfecta, es decir, no comete errores y la curva tiene un valor constante igual a 1. Por otro lado, un modelo que tenga una curva ROC igual a la bisectriz del primer cuadrante (y=x) se tratará de un modelo que siempre tiene una tasa TPR igual a su tasa FPR, es decir, que acierta lo mismo que falla y tiene un 50\% de éxito, no siendo mejor que clasificar las clases al azar. Entre estos dos extremos se hallan resto de modelos que se acercarán más a una u otra curva según su comportamiento.


\begin{figure}[hbpt]
	\centering
	 \includegraphics[scale=0.6]{Imagenes/TeoriaResultados/ROCCurve.png}
	 \caption{ Posibles salidas de una matriz de confusión binaria \cite{ROCCurveTeorica} }
	 \label{fig:CurvaROC}
\end{figure}

En la figura anterior también se introduce el término AUC (\textit{area under ROC}), un valor que permite tener un valor numérico del comportamiento del modelo, además de la interpretación visual de la curva ROC. Como indican sus siglas en inglés, este valor es el área de la curva bajo la curva ROC. Una curva para el modelo que anteriormente se explicaba como perfecto, tendría una AUC igual a 1, mientras que el modelo que se comporta como el azar, tendrá una AUC de 0.5.




La curva ROC se define a través de la clasificación de una clase con respecto a otra, sin embargo, es posible hacer cálculos para representar una clase frente al resto en un problema multiclase. El código necesario para representar estas curvas se basará en el explicado en el ejemplo de Scikit-learn \cite{ROCCurveGrafica}. Este ejemplo continúa con el modelo detector de diferentes plantas ya visto en el apartado \ref{ConfMat} y calcula la curva ROC tanto para una clase frente al resto u \textit{One vs. All} (OvA), representado en la Figura \ref{fig:CurvasROCOvR}, como para una clase frente a otra,\textit{One vs. One} (OvO), Figura \ref{fig:CurvasROCOvO}. Además, en este ejemplo se calcula los valores con promedio micro y macro, cuya diferencia es que el promedio macro da igual importancia a cada peso dentro de cada categoría, mientras que el promedio micro da igual importancia a cada peso dentro de cada muestra. No es relevante el tipo de cálculo según el promedio, puesto que ambos ofrecen el mismo resultado. \cite{MicrovsMacro}.

\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=0.5]{Imagenes/TeoriaResultados/ROCCurveEjemplo.png}
                    \caption{ Todas las curvas ROC OvR }
                    \label{fig:CurvasROCOvR}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=0.5]{Imagenes/TeoriaResultados/ROCCurveEjemploOvOALL.png}
                    \caption{ Todas las curvas ROC OvO }
                    \label{fig:CurvasROCOvO}
	 	\end{subfigure}
	 	\caption{ Ejemplos de curvas ROC para un modelo multiclase clasificador de flores } 
	 	\label{fig:EjemploROCSkicit}
\end{figure}











































\chapter{Análisis de resultados}

 
En este apartado se muestran los resultados obtenidos durante las etapas de entrenamiento de la red neuronal, comparando diferentes modelos hasta llegar a uno óptimo. Para ello se han comparado los modelos, utilizando diferentes técnicas y cambiados parámetros, siguiendo un proceso en el cual se ha continuado con los modelos con mejores resultados, hasta llegar a uno con el mejor comportamiento.

Se ha comenzado mediante la comparación de un modelo preentrenado frente a uno desde cero o \textit{scratch}, variando el tamaño del dataset y de las épocas, con la intención de explicar de esta forma las cualidades de los modelos preentrenados.

Tras esto, se han comparado los principales modelos preentrenados utilizado en la biblioteca PyTorch, los cuales se describieron en la Sección \ref{section:ModelosPreentrenados}. 

Se ha elegido el modelo preentrenado con una mejor respuesta frente a los datos de entrada, buscando un equilibrio entre exactitud de la salida y tiempo empleado. Con este modelo, se ha buscado una posible mejora mediante dos técnicas; la aumentación de datos y la variación de hiperparámetros.

Durante estas comparaciones, se han utilizado principalmente 4 gráficas. Se ha comenzado graficando la exactitud, o \textit{accuracy}, y las pérdidas, o \textit{loss}, para observar el comportamiento del modelo tanto en entrenamiento como en validación. Después, se han utilizado las gráficas de la matriz de confusión y curva ROC, las cuales aportan un mayor entendimiento de cómo es la respuesta del modelo.


Es importante resaltar, antes de comparar los resultados obtenidos, que tipo de precisión se considera buena. Una precisión del 25\% se trataría de un modelo que es capaz de clasificar el modelo con el mismo resultado que una clasificación al azar, mientras que una clasificación al 100\% sería perfecta. Entre estos dos valores, dependerá del tipo de dataset y de cómo de importante sea la clasificación, puesto que no es lo mismo un \textit{dataset} balanceado que uno con la mayor parte de imágenes en una categoría. 


En nuestro caso, siendo este un \textit{dataset} balanceado (mismo número de imágenes para cada dataset), se podrían los siguientes límites \cite{PrecisionBuena}:

\begin{itemize}
    \item Mayor a 90\% - Muy bueno
    \item Entre 70\% y 90\% - Bueno
    \item Entre 60\% y 70\% - Normal
    \item Menor a 60\% - Malo
\end{itemize}




   
\section{Red preentrenada vs. \textit{from scratch}}

Para una mayor comprensión de las diferencias entre estos modelos, se ha variado el tamaño del dataset, así como el número de épocas, cambiando la duración del entreno. Para esta comparación, se ha utilizado el modelo preentrenado Squeezenet, ya que se trata de uno de los modelos más simples y con mejor respuesta, a la vez que rápido \cite{SqueezeNetPaper} \cite{SqueezenetBetter2}. Se ha representado la precisión de validación y entrenamiento para \textit{datasets} de 100, 1000, 5000, 10000 y 15000 imágenes durante once y cincuenta épocas.


En las Figuras \ref{fig:11E} y \ref{fig:11ETrain} se han representado la precisión de validación y entrenamiento de los modelos preentrenado y desde cero para diferentes tamaños de datasets. Se puede observar como para tamaños pequeños los modelos desde cero o \textit{scratch} no son capaces de conseguir un rendimiento demasiado bueno, mientras que los modelos preentrenados utilizando el \textit{fine tunning} llegan a resultados buenos. Esto es debido a que con esta técnica, se están utilizando los pesos predeterminados con los que cuenta el modelo SqueezeNet, el cual ha sido previamente entrenado con millones de imágenes de todo tipo. Estos pesos pueden ser un buen punto de partida para entrenar otro conjunto de imágenes, como ocurre para este dataset.

Si se hubiera dado el caso en el que el número de imágenes fuera limitado (50, 100 o 200 imágenes), la técnica de \textit{fine tunning} hubiera permitido encontrar un modelo con un rendimiento bastante bueno. Sin embargo, al tener un dataset con miles de imágenes y en el cual las cuatro clases se diferencian principalmente por colores, sin demasiadas formas complejas, un modelo \textit{scratch} es capaz de entrenarse y obtener resultados parecidos a los de los modelos preentrenados. Es también relevante resaltar que, aunque el modelo desde cero consigue resultado buenos, lo hace con un mayor tiempo de procesamiento empleado, como se refleja en la Tabla \ref{tab:tiempoPreVsScratch}, y no llega a tener una precisión como tan alta como el preentrenado. Además, para el caso del entrenamiento con 15000 imágenes desde cero con 50 épocas, el entrenamiento desde cero ha tardado demasiado como para poder terminarlo y representarlo, teniendo errores durante diversos intentos de simulación.




\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/100/11/100Data11Epoch.png}
	 		\caption{ 100 imágenes }
                    \label{fig:11Epo100D}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/1000/11/1000Data11EpochL.png}
	 		\caption{ 1000 imágenes }
                    \label{fig:11Epo1000D}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/5000/11/5000Data11Epoch.png}
	 		\caption{ 5000 imágenes }
                    \label{fig:11Epo5000D}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/10000/11/10000Data11Epoch.png}
	 		\caption{ 10000 imágenes }
                    \label{fig:11Epo10000D}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/20000/11/20000Data11Epoch.png}
	 		\caption{ 15000 imágenes }
                    \label{fig:11Epo2000D}
	 	\end{subfigure}
	 	\caption{ Precisión de validación para once épocas con diferente número de imágenes }
	 	\label{fig:11E}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Tamaño dataset} & Preentrenado & \textit{Scratch}\\ \hline
100    &  0' 45''  &  0' 53''  \\ \hline
1000   &  2' 28''  &  3' 34''  \\ \hline
5000   &  11' 04''  &  11'34''\\ \hline
10000  &  27' 58''  &  28' 58''  \\ \hline
15000  &  44' 26'' &  46' 58'' \\ \hline
\end{tabular}
\caption{Tiempo de entrenamiento para tamaños de \textit{dataset}}
\label{tab:tiempoPreVsScratch}
\end{table}


\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/100/11/Training100Data11Epoch.png}
	 		\caption{ 100 imágenes }
                    \label{fig:11Epo100DTrain}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/1000/11/Training1000Data11Epoch.png}
	 		\caption{ 1000 imágenes }
                    \label{fig:11Epo1000DTrain}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/5000/11/Training5000Data11Epoch.png}
	 		\caption{ 5000 imágenes }
                    \label{fig:11Epo5000DTrain}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/10000/11/Training10000Data11Epoch.png}
	 		\caption{ 10000 imágenes }
                    \label{fig:11Epo10000DTrain}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/20000/11/Training20000Data11Epoch.png}
	 		\caption{ 20000 imágenes }
                    \label{fig:11Epo2000DTrain}
	 	\end{subfigure}
	 	\caption{ Precisión de entrenamiento para once épocas con diferente núm. de imágenes }
	 	\label{fig:11ETrain}
\end{figure}





Tras esto, se han vuelto a realizar los mismos entrenamientos para un tiempo de entrenamiento de cincuenta épocas, representados en las Figuras \ref{fig:50E} y \ref{fig:50ETrain} para validación y entrenamiento, respectivamente, confirmándose los conceptos ya vistos para once épocas. El aumento de épocas supone un aumento de tiempo de procesamiento para el entrenamiento, pero no una subida considerable en precisión, por lo que se continuará con las once épocas para el resto de apartados. Además, se continuará con un dataset de 10000 imágenes, puesto que se aprecia con estos resultados que es suficiente para conseguir buenos resultados sin consumir un tiempo exagerado en entrenamiento.







\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/100/50/100Data50Epoch.png}
	 		\caption{ 100 imágenes }
                    \label{fig:50Epo100D}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/1000/50/1000Data50Epoch.png}
	 		\caption{ 1000 imágenes }
                    \label{fig:50Epo1000D}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/5000/50/5000Data50Epoch.png}
	 		\caption{ 5000 imágenes }
                    \label{fig:50Epo5000D}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/10000/50/10000Data50Epoch.png}
	 		\caption{ 10000 imágenes }
                    \label{fig:50Epo10000D}
	 	\end{subfigure}
	 	\caption{ Precisión de validación para cincuenta épocas con diferente núm. de imágenes }
	 	\label{fig:50E}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Tamaño dataset} & Preentrenado & \textit{Scratch}\\ \hline
100    &  0' 45''  &  0' 53''  \\ \hline
1000   &  4' 3''  &  4' 33''  \\ \hline
5000   &  26' 29''  &  27' 5''\\ \hline
10000  &  36' 53''  &  46' 18''  \\ \hline
20000  &  53' 4'' &  No conseguido\\ \hline
\end{tabular}
\caption{Tiempo de entrenamiento para tamaños de \textit{dataset} con cincuenta épocas}
\label{tab:tiempoPreVsScratch50E}
\end{table}


\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/100/50/Training100Data50Epoch.png}
	 		\caption{ 100 imágenes }
                    \label{fig:50Epo100DTrain}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/1000/50/Training1000Data50Epoch.png}
	 		\caption{ 1000 imágenes }
                    \label{fig:50Epo1000DTrain}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/5000/50/Training5000Data50Epoch.png}
	 		\caption{ 5000 imágenes }
                    \label{fig:50Epo5000DTrain}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=5cm]{Imagenes/Resultados/10000/50/Training10000Data50Epoch.png}
	 		\caption{ 10000 imágenes }
                    \label{fig:50Epo10000DTrain}
	 	\end{subfigure}
	 	\caption{ Precisión de validación para cincuenta épocas con diferente núm. de imágenes }
	 	\label{fig:50ETrain}
\end{figure}


\newpage
\subsection{Conclusiones}

Se continuará con un modelo que entrene durante once épocas y 10000 imágenes en el conjunto de datos. En la Figura \ref{fig:Modelo1} se observan los resultados que ofrece este modelo, tanto su precisión y pérdida de validación y entrenamiento como su Curva ROC y matriz de confusión.

Se aprecia tanto en la matriz de confusión como en la Curva ROC que el modelo consigue reconocer la clase \textit{Empty} con una precisión de casi el 100\%, lo cual es lógico, puesto que estas imágenes están vacías y son fáciles de reconocer con respecto al resto. Sin embargo, es importante que esta clase se reconozca con facilidad para que la máquina no imprima en zonas vacías en las que no tiene sentido.

Por otro lado, es importante que el modelo sea capaz de diferenciar con precisión las imágenes \textit{Under} de las \textit{Over}, puesto que el error de clasificación más grave del modelo ocurriría cuando el modelo asigne el valor \textit{Under} a una imagen \textit{Over} y al contrario. Esto es así, puesto que si se utiliza un rellenado de huecos o lijado tras la impresión de la capa, se estaría dando una orden de rellenar huecos a una parte de la pieza que ya tiene demasiado material o lijando una parte con falta de material. Dentro de estos fallos, el lijado de una material que no existe podría dar problemas, pero lo habitual sería que no ocurriera nada. Sin embargo, el rellenado de huecos de una parte con demasiado material estaría provocando un error aún mayor al que se tenía y debe ser el que no ocurra bajo ninguna circunstancia.

\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=5cm,height=5cm]{Imagenes/Resultados/Pretrained/Squeeze/AccSqueezenet.png}
	 		\caption{ Precisión  }
                    \label{fig:Modelo11}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=5cm,height=5cm]{Imagenes/Resultados/Pretrained/Squeeze/LossSqueezenet.png}
                    \caption{ Pérdidas }
                    \label{fig:Modelo12}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=5cm]{Imagenes/Resultados/Pretrained/Squeeze/SqueezenetMatConfVal.png}
	 		\caption{ Matriz de confusión para validación }
                    \label{fig:Modelo13}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/Squeeze/ROCValidationOvRSqueezenet.png}
	 		\caption{ Curva ROC para validación OvR }
                    \label{fig:Modelo14}
	 	\end{subfigure}
	 	\caption{ Resultados del modelo preentrenado}
	 	\label{fig:Modelo1}
\end{figure}
\vspace{4cm}



\newpage
\section{Comparación de modelos preentrenados}



Se continúa la elección del modelo óptimo comparando los resultados de los principales modelos preentrenados vistos en la Sección \ref{section:ModelosPreentrenados}. Para ello, se ha graficado la precisión en validación y entrenamiento, así como la pérdida del modelo. Además, también se han representado la matriz de confusión y las Curva ROC de las clases frente al resto (Curva ROC OvR) para tener un mejor entendimiento del funcionamiento de los modelos. Se han entrenado los modelos durante 11 épocas y con 10000 imágenes tras ver que estos valores eran suficientes en el apartado anterior.




\subsection{SqueezeNet}

En la Figura \ref{fig:RSqueeze} se encuentran graficados los resultados para el entrenamiento del modelo SqueezeNet con diez mil imágenes y once épocas. En la Tabla \ref{tab:RSqueeze} se encuentran resumidos los resultados, así como el tiempo concurrido durante el entrenamiento.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Modelo} & SqueezeNet \\ \hline
Max Acc Val                                           &  82.25 \%       \\ \hline
Max Acc Train                                         &  82.09 \%      \\ \hline
Min Loss Val                                          &  0.4617     \\ \hline
Min Loss Train                                        &  0.3630    \\ \hline
Time                                                  &  26' 58''   \\ \hline
\end{tabular}
\caption{Resultados SqueezeNet}
\label{tab:RSqueeze}
\end{table}


\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/Squeeze/AccSqueezenet.png}
	 		\caption{ Precisión  }
                    \label{fig:SqueezePrecision}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/Squeeze/LossSqueezenet.png}
                    \caption{ Pérdidas }
                    \label{fig:SqueeenetLoss}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/Pretrained/Squeeze/SqueezenetMatConfVal.png}
	 		\caption{ Matriz de confusión para validación }
                    \label{fig:SqueeenetMatConf}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/Pretrained/Squeeze/ROCValidationOvRSqueezenet.png}
	 		\caption{ Curva ROC para validación }
                    \label{fig:SqueeenetCurvaROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con SqueezeNet}
	 	\label{fig:RSqueeze}
\end{figure}
\vspace{4cm}





\newpage
\vspace{4cm}
\newpage
\subsection{AlexNet}

En la Figura \ref{fig:RAlexNet} se encuentran graficados los resultados para el entrenamiento del modelo AlexNet con 10 mil imágenes y 11 épocas. En la Tabla \ref{tab:RAlexNet} se encuentran resumidos los resultados, así como el tiempo concurrido durante el entrenamiento.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Modelo} & AlexNet \\ \hline
Max Acc Val                                           &  82.5 \%      \\ \hline
Max Acc Train                                         &  92.55 \%    \\ \hline
Min Loss Val                                          &  0.5150       \\ \hline
Min Loss Train                                        &  0.2026       \\ \hline
Time                                                  &  24' 36''   \\ \hline
\end{tabular}
\caption{Resultados AlexNet}
\label{tab:RAlexNet}
\end{table}


\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/Alex/AccAlexnet.png}
	 		\caption{ Precisión }
                    \label{fig:AlexPrecision}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/Alex/LossAlexnet.png}
                    \caption{ Pérdidas }
                    \label{fig:AlexnetLoss}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/Pretrained/Alex/MatConfValAlexnet.png}
	 		\caption{ Matriz de confusión para validación }
                    \label{fig:AlexnetMatConf}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/Pretrained/Alex/ROCValidationOvRAlexnet.png}
	 		\caption{ Curva ROC para validación }
                    \label{fig:AlexnetCurvaROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con AlexNet}
	 	\label{fig:RAlexNet}
\end{figure}





\newpage
\subsection{ResNet}


En la Figura \ref{fig:RResNet} se encuentran graficados los resultados para el entrenamiento del modelo AlexNet con 10 mil imágenes y 11 épocas. En la Tabla \ref{tab:RResNet} se encuentran resumidos los resultados, así como el tiempo concurrido durante el entrenamiento.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Modelo} & ResNet \\ \hline
Max Acc Val                                           & 82.65 \%        \\ \hline
Max Acc Train                                         & 95.03 \%       \\ \hline
Min Loss Val                                          & 0.4807        \\ \hline
Min Loss Train                                        & 0.1419        \\ \hline
Time                                                  & 17' 59''        \\ \hline
\end{tabular}
\caption{Resultados ResNet}
\label{tab:RResNet}
\end{table}


\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/ResNet/AccResnet.png}
	 		\caption{ Precisión }
                    \label{fig:ResnetPrecision}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/ResNet/LossResnet.png}
                    \caption{ Pérdidas }
                    \label{fig:ResnetLoss}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/Pretrained/ResNet/MatConfValResnet.png}
	 		\caption{ Matriz de confusión para validación }
                    \label{fig:ResnetMatConf}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/Pretrained/ResNet/ROCValidationOvRResnet.png}
	 		\caption{ Curva ROC para validación }
                    \label{fig:ResnetCurvaROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con ResNet}
	 	\label{fig:RResNet}
\end{figure}




\newpage
\subsection{VGG}



En la Figura \ref{fig:RVGG} se encuentran graficados los resultados para el entrenamiento del modelo AlexNet con 10 mil imágenes y 11 épocas. En la Tabla \ref{tab:RVGG} se encuentran resumidos los resultados, así como el tiempo concurrido durante el entrenamiento.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Modelo} & VGG \\ \hline
Max Acc Val                                           & 82.75 \%        \\ \hline
Max Acc Train                                         & 97.10 \%        \\ \hline
Min Loss Val                                          & 0.4523        \\ \hline
Min Loss Train                                        & 0.088        \\ \hline
Time                                                  & 71' 06''        \\ \hline
\end{tabular}
\caption{Resultados VGG}
\label{tab:RVGG}
\end{table}


\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/VGG/AccVGG.png}
	 		\caption{ Precisión  }
                    \label{fig:VGGPrecision}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/VGG/LossVGG.png}
                    \caption{ Pérdidas }
                    \label{fig:VGGLoss}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/Pretrained/VGG/MatConfValVGG.png}
	 		\caption{ Matriz de confusión para validación }
                    \label{fig:VGGMatConf}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/Pretrained/VGG/ROCValidationOvRVGG.png}
	 		\caption{ Curva ROC para validación }
                    \label{fig:VGGCurvaROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con VGG}
	 	\label{fig:RVGG}
\end{figure}


\newpage
\subsection{DenseNet}


En la Figura \ref{fig:RDenseNet} se encuentran graficados los resultados para el entrenamiento del modelo AlexNet con 10 mil imágenes y 11 épocas. En la Tabla \ref{tab:RDenseNet} se encuentran resumidos los resultados, así como el tiempo concurrido durante el entrenamiento.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Modelo} & DenseNet \\ \hline
Max Acc Val                                           & 81.80\%        \\ \hline
Max Acc Train                                         & 90.21\%        \\ \hline
Min Loss Val                                          & 0.4859        \\ \hline
Min Loss Train                                        & 0.2686        \\ \hline
Time                                                  & 65'39''        \\ \hline
\end{tabular}
\caption{Resultados DenseNet}
\label{tab:RDenseNet}
\end{table}


\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/DenseNet/AccDensenet.png}
	 		\caption{ Precisión }
                    \label{fig:DensePrecision}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/DenseNet/LossDensenet.png}
                    \caption{ Pérdidas }
                    \label{fig:DenseLoss}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/Pretrained/DenseNet/MatConfDensenetVal.png}
	 		\caption{ Matriz de confusión para validación }
                    \label{fig:DenseMatConf}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/Pretrained/DenseNet/ROCValidationOvRDensenet.png}
	 		\caption{ Curva ROC para validación }
                    \label{fig:DensenetCurvaROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con DenseNet}
	 	\label{fig:RDenseNet}
\end{figure}


\newpage
\subsection{Inception}


En la Figura \ref{fig:RInception} se encuentran graficados los resultados para el entrenamiento del modelo Inception con 10 mil imágenes y 11 épocas. En la Tabla \ref{tab:RInception} se encuentran resumidos los resultados, así como el tiempo concurrido durante el entrenamiento.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Modelo} & Inception \\ \hline
Max Acc Val                                           & 83.85\%        \\ \hline
Max Acc Train                                         & 94.89\%        \\ \hline
Min Loss Val                                          & 0.4786       \\ \hline
Min Loss Train                                        & 0.2021        \\ \hline
Time                                                  & 42'21''        \\ \hline
\end{tabular}
\caption{Resultados Inception}
\label{tab:RInception}
\end{table}


\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/Inception/AccInception.png}
	 		\caption{ Precisión }
                    \label{fig:InceptionPrecision}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=6cm,height=6cm]{Imagenes/Resultados/Pretrained/Inception/LossInception.png}
                    \caption{ Pérdidas }
                    \label{fig:InceptionLoss}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/Pretrained/Inception/MatConfInceptionVal.png}
	 		\caption{ Matriz de confusión para validación }
                    \label{fig:InceptionConf}
	 	\end{subfigure}
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/Pretrained/Inception/ROCValidationOvRInception.png}
	 		\caption{ Curva ROC para validación }
                    \label{fig:InceptionCurvaROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con Inception}
	 	\label{fig:RInception}
\end{figure}




\subsection{Resumen de resultados y conclusiones}

Tras estudiar estos resultados, se confirman parcialmente las predicciones vistas en la teoría relativa a los diferentes modelos preentrenados. Modelos como Inception, VGG y DenseNet son capaces de obtener una mayor precisión en entrenamiento, aunque su mayor complejidad también lleva consigo un mayor tiempo de entrenamiento. Es por esto que para este dataset, modelos menos complejos suponen una mayor elección, puesto que estos son suficientes para clasificar imágenes sencillas como las presentes. 

Entre los modelos más simples, ResNet consigue precisiones de validación y de entrenamiento altas, manteniendo el tiempo de ejecución más bajo. Además, la curva ROC muestra una muy buena clasificación, especialmente para las clases \textit{Over} y \textit{Under}. Prestando especial atención a las imágenes con \textit{Over} clasificadas como \textit{Under} (este es el peor de los fallos posibles), se observa en la matriz de confusión que ResNet hace un muy buen trabajo en esta clasificación y cuenta con el menor número de falsos positivos de este tipo. 

Por estas razones, se ha continuado con el modelo ResNet para el resto de experimentos. Cabe destacar también que todos los problemas sufren de \textit{overfitting}, lo que se puede detectar tanto en las curvas de precisión como perdida, por lo que este fallo se tratará de solucionar con las siguientes modificaciones.







\newpage
\section{Aumentación de datos}


Se aplicarán diferentes transformaciones a las imágenes, según lo visto en el apartado \ref{AumentacionDatos}, para comprobar si se corrige el \textit{overfitting} del modelo y se mejora la validación, especialmente la clasificación entre clases que no deben ser confundidas. Para ello, se han utilizado transformaciones disponibles en PyTorch, así como en la librería imgaug, la cual se especializa en transformaciones en imágenes. Cabe destacar que para estudiar los cambios provocador por estas transformaciones, se ha representado la matriz de confusión y la curva ROC para la clase \textit{Under} sobre \textit{Over}, ya que este es el peor de los errores posibles y el que se pretende solucionar con las transformaciones.


\subsection{Transformación en tamaño y giro}


Se comienza con un pequeño giro de 20º y un aumento de las imágenes. En la Figura \ref{fig:DataAugmenEjPytorchSimple} se han representado las imágenes transformadas para comprobar los efectos del cambio, y en las Figuras \ref{fig:DataAugmenEjPytorchSimpleMatConf} y \ref{fig:DataAugmenEjPytorchSimpleROC} se han representado la matriz de confusión y curva ROC 0v0 para la clase Under vs. Over, respectivamente.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{Imagenes/Resultados/DataAugmentation/PytorchPoco/EjemploImagenPyTorchSimple.png}
    \caption{ Imágenes transformadas con algunas transformaciones simples }
    \label{fig:DataAugmenEjPytorchSimple}
\end{figure}



\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/DataAugmentation/PytorchPoco/PocoPyTorchMatConf2242.png}
	 		\caption{ Matriz de confusión }
                    \label{fig:DataAugmenEjPytorchSimpleMatConf}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/DataAugmentation/PytorchPoco/PocoPytorchROC224.png}
                    \caption{ Curva ROC }
                    \label{fig:DataAugmenEjPytorchSimpleROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con algunas transformaciones simple}
	 	\label{fig:DataAugmentationPytorchSimpleResults}
\end{figure}



\subsection{Transformaciones complejas PyTorch}


Tras esto, se han aumentado las transformaciones, añadiendo algunas más complejas. Se han utilizado transformaciones para cambiar el brillo, contraste, saturación, tono y tamaño, de la imagen. Se ha aplicado también un giro horizontal a algunas imágenes y un giro de 45º, además de un desenfoque gaussiano. En la Figura \ref{fig:DataAugmenEjPytorchComp} se han representado los efectos de estos cambios aplicados en las imágenes y los resultados en las Figuras \ref{fig:DataAugmenEjPytorchCompleMatConf} y \ref{fig:DataAugmenEjPytorchSimpleROC}.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Imagenes/Resultados/DataAugmentation/PyTorchMucho/EjemploImagenPyTorchComplicado.png}
    \caption{ Imágenes transformadas con algunas transformaciones simples de PyTorch }
    \label{fig:DataAugmenEjPytorchComp}
\end{figure}



\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=8cm,height=7cm]{Imagenes/Resultados/DataAugmentation/PyTorchMucho/ToLaPescaPyTorchMatConf78.png}
	 		\caption{ Matriz de confusión }
                    \label{fig:DataAugmenEjPytorchCompleMatConf}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=8cm,height=8cm]{Imagenes/Resultados/DataAugmentation/PyTorchMucho/ToLaPescaPyTorchROC78.png}
                    \caption{ Curva ROC }
                    \label{fig:DataAugmenEjPytorchCompROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con algunas transformaciones complejas de PyTorch}
	 	\label{fig:DataAugmentationPytorchCompleResults}
\end{figure}




\subsection{Transformaciones complejas librería imgaug}


Por último, se ha recurrido a la biblioteca pública Imgaug, la cual permite transformaciones más complejas de las disponibles en PyTorch. Se han hecho algunas modificaciones como el desenfoque gausiano, rotación, eliminar píxeles o saturar la imagen. Además, esta biblioteca permite algunos comportamientos, como el comando ``sometimes'' para solo afectar a algunas imágenes del dataset con la probabilidad deseada.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{Imagenes/Resultados/DataAugmentation/IAA/EjemploImagenIAAA.png}
    \caption{ Imágenes transformadas con transformaciones de la biblioteca Imgaug }
    \label{fig:DataAugmenEjIAA}
\end{figure}



\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=8cm,height=7cm]{Imagenes/Resultados/DataAugmentation/IAA/ToLaPescaMatConf224.png}
	 		\caption{ Matriz de confusión }
                    \label{fig:DataAugmenEjIAAComple}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=8cm,height=8cm]{Imagenes/Resultados/DataAugmentation/IAA/ToLaPescaROC224.png}
                    \caption{ Curva ROC }
                    \label{fig:DataAugmenEjIAAROC}
	 	\end{subfigure}
	 	\caption{ Resultados del entrenamiento con algunas transformaciones de la biblioteca Imgaug }
	 	\label{fig:DataAugmentationIAAeResults}
\end{figure}



\subsection{Resumen de resultados y conclusiones}


En la Tabla \ref{tab:ResumenResultadosDataAUG} se resume el rendimiento con las diferentes transformaciones a imágenes. Al estudiar esta tabla, así como los resultados y gráficas anteriores, se puede concluir que la aumentación de datos no supone una mejora relevante para este modelo, ya que los modelos con mayores transformaciones no han solucionado problemas de clasificación ni de rendimiento general del modelo. Se puede razonar la razón tras estos hechos, ya que el \textit{dataset} con el que se trabaja ya cuenta con suficientes imágenes como para ofrecer el buen resultado con el que contábamos de casi un 83 \% en validación. Además, imágenes como las que se encuentran en este conjunto de datos ya son lo suficientemente simples, puesto que la clasificación se basa en detectar los colores verde, rojo y azul. Otros conjuntos de datos se podrían beneficiar de este tipo de complejas transformaciones si las imágenes cuentan con demasiados detalles que el modelo no es capaz de detectar, como imágenes reales, u otras en las que debe detectar elementos muy particulares, como detección de tumores.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} Transformaciones } & Max Acc Val & Max Acc Train  & Tiempo  \\ \hline
Trans. simples PyTorch                              & 83.25\%  &  85.85\%  & 24'28''      \\ \hline
Trans. complejas PyTorch                            & 83.15\%   &  84.41\%  & 42'12''     \\ \hline
Trans. Imgaug                                       & 83.00\%   &  86.1\%  & 63'56''    \\ \hline
\end{tabular}
\caption{Resultados entrenamientos para aumentación del conjunto de datos}
\label{tab:ResumenResultadosDataAUG}
\end{table}


En definitiva, se escogerán las transformaciones simples de PyTorch, puesto que estas no han supuesto una pérdida aún mayor de tiempo de entrenamiento como las otras dos y han dado el mejor resultado.





\newpage
\section{Variación de hiperparámetros}

Como se ha explicado en la sección \ref{Hiperparametrosteoria} existen dos parámetros a optimizar en este apartado; el valor de la tasa de aprendizaje inicial y la variación de la tasa de aprendizaje en el entrenamiento. Se ha comenzado entrenando el modelo con diferentes funciones de optimización y diferentes valores de tasa de aprendizaje (lr) para asegurar con qué valor funciona mejor el modelo y empezar con este. Tras esto, se han utilizado las funciones StepLR y ExponentialLR para variar la tasa de aprendizaje y comprobar con qué valores de variación el modelo aprende mejor. Cabe destacar que no se ha considerado el tiempo de ejecución a la hora de considerar el mejor optimizador, ya que todos han ofrecido resultados temporales parecidos durante las simulaciones.



\subsection{Tasa de aprendizaje inicial}

Se recomienda utilizar una tasa de aprendizaje de entre 0.1 y 0.001 como comienzo y comprobar si el problema mejora o no con una menor \cite{LearningRate}, por lo que se ha comenzado con una tasa de aprendizaje de 0.1 y se ha disminuido hasta comprobar que el modelo no mejoraba.


\subsubsection{SGD}

En la Figura \ref{fig:Hiperparametros1SGD} se han utilizado las tasas de aprendizaje 0.1, 0.05, 0.005 y 0.0005, hasta comprobar que el modelo no mejoraba. La mejor precisión de validación se ha conseguido con una tasa de 0.005, que será la que se utilice en el próximo apartado.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{Imagenes/Resultados/Hiperparámetros/Hiper1SGD.png}
    \caption{ Precisión de validación con el optimizador SGD y diferentes tasas de aprendizaje }
    \label{fig:Hiperparametros1SGD}
\end{figure}


\subsubsection{Adam}

Se han utilizado las tasas de aprendizaje 0.1, 0.05 y 0.005, finalizando en este valor, ya que, como se comprueba en la Figura \ref{fig:Hiperparametros1Adam}, se consiguen los mejores resultados con un valor de 0.1.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.65]{Imagenes/Resultados/Hiperparámetros/Hiper1Adam.png}
    \caption{ Precisión de validación con el optimizador Adam y diferentes tasas de aprendizaje }
    \label{fig:Hiperparametros1Adam}
\end{figure}

\subsubsection{Adagrad}

Al igual que con el optimizador Adam, se ha conseguido la mejor respuesta con $lr=0.05$, habiendo hecho pruebas con 0.1, 0.05, 0.005, 0.0005, graficados en la Figura \ref{fig:Hiperparametros1Adagrad}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.65]{Imagenes/Resultados/Hiperparámetros/Hiper1Adagrad.png}
    \caption{ Precisión de validación con el optimizador Adagrad y diferentes tasas de aprendizaje }
    \label{fig:Hiperparametros1Adagrad}
\end{figure}

\subsubsection{Adadelta}

En la Figura \ref{fig:Hiperparametros1Adadelta} se observa que se han utilizado las tasas de aprendizaje 0.1, 0.05, y 0.005, hasta comprobar que el modelo no mejoraba, siendo 0.05 la mejor. Además, debido a otros proyectos donde Adadelta tiene buenos resultados con tasas de aprendizajes altas, se han intentado con $lr=10$, aunque la respuesta no era mejor.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{Imagenes/Resultados/Hiperparámetros/Hiper1Adadelta.png}
    \caption{ Precisión de validación con el optimizador Adadelta y diferentes tasas de aprendizaje }
    \label{fig:Hiperparametros1Adadelta}
\end{figure}

\subsubsection{Resumen de resultados}

En la Tabla \ref{tab:Hiperparametros1} se observan resumidos los mejores resultados de precisión de validación obtenidos por cada modelo.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Optimización } & lr = 0.1 & lr = 0.05  & lr = 0.005 & lr = 0.0005 & lr = 10 \\ \hline
SGD & 83.00\%  &  83.15\%  & 83.40\%   & 82.10\%  & - \\ \hline
Adam   & 84.05\%   &  83.05\%  & 82.85\%  & -\% & -  \\ \hline
Adagrad & 83.10\%   &  83.30\%  & 83.45\%  & 81.55\% & -\\ \hline
Adadelta  & 83.20\%   &  83.30\%  & 81.45\% & - & 82.80\%  \\ \hline
\end{tabular}
\caption{Resultados entrenamientos para aumentación del conjunto de datos}
\label{tab:Hiperparametros1}
\end{table}





\subsection{Variación de tasa de aprendizaje}

A continuación, se han resumido los resultados obtenidos para cada optimizador con diferentes variaciones de tasa de aprendizaje. Para todos los casos se han utilizado las dos funciones de variación de tasa de aprendizaje StepLR y ExponentialLR vistos en el apartado \ref{VariacionLearningRate}, utilizando como tasa de aprendizaje inicial la que haya ofrecido un mejor resultado en la sección anterior y variándola de diferente forma. En estas simulaciones, se ha cambiado el número de épocas a treinta para poder comprobar con mayor facilidad si el modelo mejora con la variación de tasa de aprendizaje.


\subsubsection{SGD}

Se ha comenzado utilizando la función StepLR con un valor inicial de tasa de aprendizaje de 0.005 y de 0.3 para gamma ($\gamma$), reduciendo la tasa de aprendizaje cada dos épocas, y manteniendo el $momentum$ constante en 0.9 para esta y todas las simulaciones. En esta primera simulación, el modelo demuestra que los cambios son demasiados grandes como para mejorar, por lo que se ha vuelto a probar con una tasa de aprendizaje de 0.005. Se ha cambiado la variación a cada 3 épocas, y se ha aumentado $\gamma$ a 0.6, reduciendo así la variación de la tasa de aprendizaje. Este modelo tampoco aporta mejorar con respecto a mantener la tasa constante.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & N.º épocas para cambio & Tasa de aprendizaje & Val. Acc. \\ \hline
StepLR & 0.3  & 2 & 0.005 & 82.15\% \\ \hline
StepLR & 0.6  & 3 & 0.005 & 83.20\% \\ \hline
\end{tabular}
\caption{Resultados para SGD con varianza de la tasa de aprendizaje escalada}
\label{tab:VarStepLRSGD}
\end{table}


A continuación, se ha utilizado la función ExponentialLR, variando la tasa de aprendizaje exponencialmente con $\gamma = 0.6$. De nuevo, el resultado es parecido al conseguido con la tasa de aprendizaje constante, sin mejorarlo,

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & Tasa de aprendizaje & Val. Acc. \\ \hline
ExponentialLR & 0.6  & 0.005 & 83.35\% \\ \hline
\end{tabular}
\caption{Resultados para SGD con varianza de la tasa de aprendizaje exponencial}
\label{tab:VarExpLRSGD}
\end{table}




\subsubsection{Adam}

En este caso, se ha comenzado con un valor de 0.1 para la tasa de aprendizaje, puesto que era el valor que ofrecía una mejor precisión en el apartado anterior. Se han utilizado descensos con StepLR de $\gamma = 0.1$  y $\gamma = 0.8$ cada 3 y 5 épocas, de forma que se tenga un descenso más rápido y otro más lento. Por último, también se prueba con una tasa de aprendizaje inicial de 0.005, ya que esta tenía buenos resultados. Se le aplica un descenso escalonado con $\gamma = 0.1$ cada 3 épocas y se consigue una precisión superior a la constante con esta tasa de aprendizaje, pero no supera a la de tasa de aprendizaje en 0.1.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & N.º épocas para cambio & Tasa de aprendizaje & Val. Acc. \\ \hline
StepLR & 0.1  & 3 & 0.1 & 83.70\% \\ \hline
StepLR & 0.8  & 5 & 0.1 & 83.10\% \\ \hline
StepLR & 0.1  & 3 & 0.005 & 83.90\% \\ \hline
\end{tabular}
\caption{Resultados para Adam con varianza de la tasa de aprendizaje escalada}
\label{tab:VarStepLRAdam}
\end{table}

En ninguno de los casos anteriores se consigue una precisión superior a la conseguida con la tasa de aprendizaje constante, probando además con una bajada exponencial con $\gamma = 0.1$.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & Tasa de aprendizaje & Val. Acc. \\ \hline
ExponentialLR & 0.1  & 0.005 & 83.40\% \\ \hline
\end{tabular}
\caption{Resultados para Adam con varianza de la tasa de aprendizaje exponencial}
\label{tab:VarExpLRAdam}
\end{table}




\subsubsection{Adagrad}

Se han utilizado dos tasas de aprendizaje de partida, 0.05 y 0.005, puesto que ambas tenían resultados parecidos en el apartado anterior, aplicando para ambas un descenso cada 5 épocas de $\gamma = 0.1$. En ambos casos, se ha conseguido mejorar la precisión que tenía el modelo con tasa de aprendizaje fija levemente.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & N.º épocas para cambio & Tasa de aprendizaje & Val. Acc. \\ \hline
StepLR & 0.1  & 5 & 0.05 & 83.50\% \\ \hline
StepLR & 0.1  & 5 &  0.005 & 83.75 \\ \hline
\end{tabular}
\caption{Resultados para Adagrad con varianza de la tasa de aprendizaje escalada}
\label{tab:VarStepLRAdagrad}
\end{table}

Además, se ha utilizado una bajada exponencial con $\gamma = 0.9$ y tasa de aprendizaje inicial 0.005, consiguiendo la mejor precisión para el optimizador Adagrad hasta ahora, 83.95\%.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & Tasa de aprendizaje & Val. Acc. \\ \hline
ExponentialLR & 0.1  & 0.005 & 83.95\% \\ \hline
\end{tabular}
\caption{Resultados para Adagrad con varianza de la tasa de aprendizaje exponencial}
\label{tab:VarExpLRAdagrad}
\end{table}



\subsubsection{Adadelta}

Se ha comenzado utilizando una tasa de aprendizaje inicial de 0.05, puesto que es la que ofrece una mejor respuesta, con $\gamma = 0.1$ cada 3 épocas, consiguiendo una precisión algo superior a la vista con la tasa fija. Al comprobar que el modelo mejoraba con menores tasas, se ha vuelto a probar con una tasa de aprendizaje de 0.005, en este caso variando la tasa cada 3 épocas con $\gamma = 0.6$, consiguiendo la mejor precisión con el optimizador Adadelta.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & N.º épocas para cambio & Tasa de aprendizaje & Val. Acc. \\ \hline
StepLR & 0.1  & 3 & 0.05 & 83.35\% \\ \hline
StepLR & 0.6  & 3 &  0.005 & 83.75 \\ \hline
\end{tabular}
\caption{Resultados para Adadelta con varianza de la tasa de aprendizaje escalada}
\label{tab:VarStepLRAdadelta}
\end{table}

También se ha probado una bajada exponencial con $\gamma = 0.1$ y tasa de aprendizaje inicial 0.05.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000}Func. Varianza } & $\gamma$ & Tasa de aprendizaje & Val. Acc. \\ \hline
ExponentialLR & 0.1  & 0.05 & 83.38\% \\ \hline
\end{tabular}
\caption{Resultados para Adadelta con varianza de la tasa de aprendizaje exponencial}
\label{tab:VarExpLRAdadelta}
\end{table}


\subsection{Conclusiones}

Se puede observar que en la mayoría de los casos anteriores, una variación de la tasa de aprendizaje durante el entrenamiento mejora la respuesta del modelo, aunque en estos casos, esta mejora no es de un gran nivel. Se puede concluir que el modelo ya se encuentra en un punto en el que es complicado ofrecer mejoras, con lo que aunque las mejoras sean pequeñas, estas pequeñas subidas de precisión no son triviales. Para mejorar la respuesta del modelo se debería recurrir a otras estrategias como aumentar y mejorar el dataset o la validación cruzada.

Se ha decidido considerar como óptimo el modelo con el optimizador Adagrad y una bajada dela tasa de aprendizaje exponencial, puesto que consigue una buena precisión en todos sus resultados, llegando a ofrecer casi un 84\%. Además, se han graficado sus curvas ROC y matriz de confusión (ver Figura \ref{fig:HiperFinal}), donde se observa que cumple de gran forma al reconocer la clase \textit{Under} frente a la clase \textit{Over}.

\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/DataAugmentation/PyTorchMucho/ToLaPescaPyTorchMatConf78.png}
	 		\caption{ Matriz de confusión }
                    \label{fig:HiperFinalMatConf}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/DataAugmentation/PyTorchMucho/ToLaPescaPyTorchROC78.png}
                    \caption{ Curva ROC OvR}
                    \label{fig:HiperFinalROCOvR}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/DataAugmentation/PyTorchMucho/ToLaPescaPyTorchROC78.png}
                    \caption{ Curva OvO para \textit{Under} frente a \textit{Over} }
                    \label{fig:HiperFinalROCOvO}
	 	\end{subfigure}
	 	\caption{ Matriz de confusión y curvas ROC para modelo óptimo tras ajuste de hiperparámetros }
	 	\label{fig:HiperFinal}
\end{figure}

\newpage
\section{Validación cruzada}


Como se ha explicado en la sección \ref{TiposValidacionCruzada} se utilizarán las técnicas de validación cruzada aleatoria repetida, temporal y \textit{k-folds}. Los códigos utilizados son parecidos a los usados en los apartados anteriores y para añadir la validación cruzada se han consultado los códigos que se encuentran en \cite{CVGithub}. Además, los códigos están explicados en el Anexo de este texto.

\subsection{Aleatoria}

Se comienza por la validación cruzada aleatoria, puesto que es la más simple, no siendo más que la validación \textit{hold out} repetida durante diversas iteraciones, eligiendo diferentes imágenes del conjunto de datos al azar en cada una. La simpleza de este método se aprecia en la Figura \ref{fig:CVAlea} donde se pueden ver los resultados mediante la matriz de confusión y curvas ROC para OvR (\textit{One vs. Rest}) y OvO (\textit{One vs. One}) para la situación crítica de \textit{Under vs. Over}. Además, se ha añadido una gráfica donde se dibujan las precisiones durante las épocas de cada división, iteración o \textit{split} del modelo.

Se observa que el modelo tiene buenos resultados, aunque no consigue superar la precisión que ya ofrecían los modelos sin validación cruzada. Sin embargo, en las curvas ROC se aprecia como el modelo es capaz de mejorar la clasificación de la situación crítica (\textit{Under vs. Over}), lo cual puede ser debido a que en este caso el modelo está viendo un mayor número de imágenes, con lo que es probable que consiga diferenciar las características de estas clases con una mayor facilidad. También se puede ver que las divisiones tienen prácticamente las mismas precisiones, lo cual es lógico dado que las divisiones son aleatorias.

\vspace{0.5cm}
\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/CrossValidation/Aleatorio/AccAleat.png}
	 		\caption{ Precisión en las iteraciones }
                    \label{fig:CVAleaAcc}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/CrossValidation/Aleatorio/MatCofAleat.png}
	 		\caption{ Matriz de confusión }
                    \label{fig:CVAleaMatConf}
	 	\end{subfigure}
\end{figure}
\vspace{0.5cm}
\begin{figure}[H]\ContinuedFloat
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/CrossValidation/Aleatorio/ROCOvRAleat.png}
                    \caption{ Curva ROC OvR}
                    \label{fig:CVAleaROCOvR}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/DataAugmentation/PyTorchMucho/ToLaPescaPyTorchROC78.png}
                    \caption{ Curva OvO para \textit{Under} frente a \textit{Over} }
                    \label{fig:CVAleaROCOvO}
	 	\end{subfigure}
	 	\caption{ Precisión, matriz de confusión y curvas ROC para modelo con validación cruzada aleatoria }
	 	\label{fig:CVAlea}
\end{figure}


\vspace{0.5cm}
\subsection{Temporal}

A continuación, se ha utilizado la validación cruzada mediante series temporales, donde aumenta el tamaño del \textit{dataset} utilizado, tanto en entrenamiento como en validación, para cada iteración. En la Figura \ref{fig:CVTime} se aprecia como en las primeras iteraciones la precisión es algo menor a lo esperado, lo cual tiene sentido, dado que durante estas iteraciones el modelo se comporta como si se estuviera entrenando con un conjunto de datos menor. Si bien el modelo consigue una buena precisión, es visible tanto en la matriz de confusión como en las curvas ROC que el modelo no consigue clasificar las imágenes tan bien como la validación cruzada aleatoria. Este tipo de validación cruzada tiene un menor coste de computación y puede ser aplicable con buenos resultados para otro tipo de modelo y conjunto de datos, pero en el caso actual no es una buena solución. Sin embargo, sí que demuestra que el modelo no necesita de un conjunto de datos tan grande como el que se está utilizando para conseguir una buena precisión, si se utiliza validación cruzada, dado que en la segunda o tercera iteración, donde el \textit{dataset} es menor, ya se consigue una buena precisión. 


\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/CrossValidation/TimeSeries/AccTime.png}
	 		\caption{ Precisión en las iteraciones }
                    \label{fig:CVTimeAcc}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/CrossValidation/TimeSeries/MatConfTime.png}
	 		\caption{ Matriz de confusión }
                    \label{fig:CVTimeMatConf}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/CrossValidation/TimeSeries/ROCOvRTime.png}
                    \caption{ Curva ROC OvR}
                    \label{fig:CVTimeROCOvR}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/CrossValidation/TimeSeries/ROCOvOTime.png}
                    \caption{ Curva OvO para \textit{Under} frente a \textit{Over} }
                    \label{fig:CVTimeROCOvO}
	 	\end{subfigure}
	 	\caption{ Precisión, matriz de confusión y curvas ROC para modelo con validación cruzada con series de tiempo }
	 	\label{fig:CVTime}
\end{figure}




\subsection{\textit{K-folds}}

Por último, se ha utilizado la validación cruzada mediante \textit{k-folds}, donde se divide el conjunto de entrenamiento en las divisiones deseadas y cada una de ellas actuará como conjunto de validación en una iteración. En la Figura \ref{fig:CVKFolds} se aprecia como la precisión es muy buena, pero parecida a las conseguidas anteriormente. Sin embargo, donde esta validación se diferencia al resto, como el mejor modelo hasta ahora es en la capacidad de clasificación que se demuestra en la matriz de confusión y curvas ROC. Como se puede apreciar en estas, se ha conseguido el mejor resultado hasta ahora para la clase \textit{Under} frente a \textit{Over}, que es precisamente uno de los principales objetivos que se buscan. Al igual que con la validación aleatoria, esta mejora se puede deber a que ahora se están utilizando más imágenes en validación y el modelo puede aprender las características de cada clase mejor. De esta forma, se comprueban las ventajas de la validación cruzada, y en concreto, de la validación cruzada con \textit{k-folds}.

\vspace{0.4cm}
\begin{figure}[H]
	 	\centering
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/CrossValidation/KFolds/AccKFold.png}
	 		\caption{ Precisión en las iteraciones }
                    \label{fig:CVKFoldsAcc}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=6cm]{Imagenes/Resultados/CrossValidation/KFolds/MatConfKFold.png}
	 		\caption{ Matriz de confusión }
                    \label{fig:CVKFoldsMatConf}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/CrossValidation/KFolds/ROCOvRKFold.png}
                    \caption{ Curva ROC OvR}
                    \label{fig:CVKFoldsROCOvR}
	 	\end{subfigure}
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[width=7cm,height=7cm]{Imagenes/Resultados/CrossValidation/KFolds/ROCOvOKFold.png}
                    \caption{ Curva OvO para \textit{Under} frente a \textit{Over} }
                    \label{fig:CVKFoldsROCOvO}
	 	\end{subfigure}
	 	\caption{ Precisión, matriz de confusión y curvas ROC para modelo con validación cruzada \textit{k-folds} }
	 	\label{fig:CVKFolds}
\end{figure}

\newpage
\section{Discusión}

En este último apartado, se discutirán los diferentes modelos probados, recapitulando las técnicas utilizadas y resultados obtenidos. Además, se recogerán y mostrarán algunos ejemplos de imágenes mal clasificadas por parte del modelo, de forma que se puedan encontrar qué tipo de imágenes el modelo no es capaz de reconocer para intentar buscar maneras de mejorarlo.


Se comenzó comparando modelos desde cero o \textit{scratch} frente a modelos preentrenados, donde se comprobó que los modelos preentrenados actuaban mejor que los \textit{scratch} para un menor número de imágenes. Además, los modelos preentrenados también conseguían mejores resultados, especialmente en cuanto a velocidad de entrenamiento frente a los \textit{Scratch}, gracias al concepto de aprendizaje transferido. Para estas simulaciones se utilizó el modelo SqueezeNet, puesto que era uno de los más simples de los modelos preentrenados.

Tras esto, se compararon los principales modelos preentrenados basados en redes neuronales convolucionales: AlexNet, ResNet, SqueezeNet, VGG, Inception y DenseNet. Se realizaron simulaciones con todos ellos y se decidió que el modelo preentrenado ResNet era el que ofrecía mejores resultados, por lo que se continuó utilizándolo durante el resto de secciones. De estos experimentos se puede concluir que los modelos simples ofrecían resultados suficientemente buenos debido a que las imágenes del \textit{dataset} son bastante sencillas, diferenciándose entre clases principalmente por colores.

En el siguiente apartado se realizaron experimentos basándose en la aumentación de datos, para lo que se sometieron las imágenes a tres tipos de transformaciones. Se comenzó por algunas simples propias de la librería PyTorch, como giros o recortes, seguidas de otras más complejas de la librería PyTorch, como desenfoques gaussianos o  cambios en el brillo, contraste o saturación. Por último, se utilizaron otras disponibles en la librería Imgaug, como la eliminación de píxeles al azar. En este caso, se comprobó que con las transformaciones simples de PyTorch era como el modelo conseguía un mejor resultado, probablemente debido de nuevo a la simpleza de las imágenes, que no necesitan grandes transformaciones para que el modelo reconozco las características de cada clase.


A continuación, se utilizaron diferentes modelos con diferentes valores de tasa de aprendizaje y funciones de optimización. En este caso, se comprobó como los cambios de estos valores pueden provocar que el modelo consiga mejores o peores resultados, si bien no se consiguió mejorar el resultado en una gran medida, dado que ya estábamos entrenando al modelo con valores de tasa de aprendizaje y función de optimización que ofrecían una gran precisión.


Por último, se ha utilizado los tipos de validación cruzada: aleatoria repetida, series temporales y \textit{k-folds}. Se ha comprobado como la validación cruzada puede dar mejores resultados que una validación de tipo \textit{hold out}, al exponer al modelo a más imágenes, consiguiendo los mejores resultados de todos los entrenamientos con la validación cruzada \textit{k-folds}.




\subsection{Imágenes mal clasificadas}

Para terminar con esta sección dedicada a los resultados, se ha decidido que es de interés guardar las imágenes mal clasificadas por el modelo y mostrarlas, de modo que se pueda tener una mejor idea de qué tipo de imágenes el modelo tiene problemas para reconocer y así poder mejorar la precisión. De esta forma, en las Figuras \ref{fig:ImMalClasifOk}, \ref{fig:ImMalClasifUnder} y \ref{fig:ImMalClasifOver} se pueden ver las imágenes mal clasificadas cuyo valor real es \textit{Ok}, \textit{Under} y \textit{Over}, respectivamente. 



Comenzando por las imágenes \textit{Ok} en la Figura \ref{fig:ImMalClasifOk}, se puede ver que la mayoría de estos fallos se encuentran en la forma de clasificar las imágenes del \textit{dataset}, es decir, en las etiquetas verdaderas dadas por los profesionales que han creado el \textit{dataset}. Muchas de estas Figuras están bastante cerca de lo que se podría considerar \textit{Under} u \textit{Over}, lo que se reconoce por los colores, pero la imagen ha sido clasificada como \textit{Ok}. En este caso, no supone un problema que el modelo tenga otro umbral diferente al establecido al realizar la clasificación, puesto que el modelo estaría estableciendo como \textit{Over} o sobreimpresas partes de la Figura que, aunque no establecidas como tal, no serían afectadas por un postprocesado (incluso beneficiándose). 


Otro tipo de fallos que también se verán más adelante son los fallos debido a imágenes complicadas de clasificar. Un ejemplo de esta es la primera subfigura de la Figura \ref{fig:ImMalClasifOk}, donde el modelo la ha clasificado como \textit{Under} debido a una clara zona donde se necesita más material. Sin embargo, la superficie cuenta también con una parte sobreimpresa. En este caso, este tipo de imágenes con superficies tan irregulares, debería dividirse en partes más pequeñas donde el proceso que necesiten sea claro. No es demasiado preocupante si las imágenes se clasifican bien por parte del modelo, puesto que se le podría aplicar un postprocesado y, en una segunda revisión, otro. 

En el caso de esta primera subfigura de la Figura \ref{fig:ImMalClasifOk} también existe otro problema y es que la etiqueta verdadera dada por parte del \textit{dataset} es \textit{Ok}, cuando la Figura claramente no dispone de un buen acabado. En este caso, se comprueba que hay casos en el que las etiquetas verdaderas dadas por los profesionales no son correctas, lo cual es comprensible, teniendo en cuenta que se trata de un conjunto de más de 40 mil imágenes, pero se debe tener en cuenta al ver la precisión del modelo.


Por último, en estas imágenes también se comprueba un ejemplo de una imagen supuesta por el modelo como \textit{Empty}, cuando realmente es \textit{Ok} debido a una pequeña zona donde existe superficie. Este tipo de fallos, no suponen ningún tipo de relevancia a la hora de usar este modelo para encontrar defectos en superficies, pero debe tenerse en cuenta que pueden existir al contar los fallos dados por el modelo.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{Imagenes/Resultados/ImMalClasif/MalClasifOK.png}
    \caption{ Imágenes \textit{Ok} mal clasificadas  }
    \label{fig:ImMalClasifOk}
\end{figure}



A continuación, se continúa con imágenes mal clasificadas del tipo \textit{Under}, es decir, en las que falta impresión y necesitarían un llenado, mostradas en la Figura \ref{fig:ImMalClasifUnder}. Al igual que en los casos anteriores, en estas imágenes se puede ver como la mayoría de imágenes son complicadas de clasificar, puesto que son muy irregulares, con partes con falta de material y otras con una sobreimpresión muy elevada. En estos casos, el modelo clasifica como \textit{Over} cuando la etiqueta verdadera es \textit{Under}, aunque ambos son correctos y se necesitaría una división de estas imágenes en otras menores con más regularidad en la superficie. De cualquier manera, en estos casos se podría realizar el postprocesado según lo que ha reconocido el modelo y tras esto hacer un segundo reconocimiento donde el modelo reconocerá el procesamiento necesario, realizando un segundo postprocesamiento.

Al igual que con las imágenes de tipo \textit{Ok} mal clasificadas, también existen casos en estas imágenes donde el límite para clasificarlas de una u otra forma es la causa del error. En estos casos no supondría un problema, ya que el modelo clasifica como \textit{Ok} imágenes que son \textit{Under}, pero cuya falta de material es muy baja.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{Imagenes/Resultados/ImMalClasif/MalClasifUnder.png}
    \caption{ Imágenes \textit{Under} mal clasificadas  }
    \label{fig:ImMalClasifUnder}
\end{figure}


Por último, en la Figura \ref{fig:ImMalClasifOver} se pueden ver imágenes de tipo \text{Over} mal clasificadas. Cabe destacar en este punto que no se han mostrado imágenes de tipo \textit{Empty} mal clasificadas, puesto que, como se ha visto en los resultados del modelo, la clasificación de esta es prácticamente perfecta. Tan solo existe problema con la clasificación \textit{Empty} cuando el modelo clasifica como \textit{Empty} imágenes donde la mayor parte de la superficie está vacía con algo de superficie en los bordes. En estos casos, se trata de un problema de elegir la etiqueta para estas imágenes en un principio y no de clasificación del modelo.

Volviendo a las imágenes de la clase \textit{Over} mal clasificadas, se puede ver que ocurren las mismas dos casuísticas que en las clases anteriores: existen imágenes complicadas para el modelo debido a la irregularidad, con zonas de sobreimpresión y falta de material, y existen imágenes al límite entre clases que el modelo clasifica como una y es otra, no siendo esto un problema debido a que es una cuestión del umbral a la hora de elegir una u otra clase. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{Imagenes/Resultados/ImMalClasif/MalClasifOver.png}
    \caption{ Imágenes \textit{Over} mal clasificadas  }
    \label{fig:ImMalClasifOver}
\end{figure}



Como se han visto en los casos anteriores, el modelo comete errores, sobre todo, en imágenes irregulares con zonas de diferentes tipos y que el modelo elige como una clase equivocada, viendo un ejemplo en la Figura \ref{fig:TipoMalClasificadaIrregular}. En estos casos, el error se solucionaría dividiendo la superficie totol impresa en zonas más pequeñas, de modo que estos cambios drásticos se solucionen. Por otra parte, si no se puede cambiar el dataset, se podría tomar una solución a la hora de realizar el postprocesamiento, realizando dos etapas de clasificación de la superficie y postprocesado, asegurando que en el caso de imágenes con dos tipos de zonas, ambas se puedan tratar.


\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{Imagenes/Resultados/ImMalClasif/Tipos/MalClasifIrregular.png}
    \caption{ Imagen mal clasificada del tipo zona irregular  }
    \label{fig:TipoMalClasificadaIrregular}
\end{figure}


En la Figura \ref{fig:MalClasificadaLimite} se puede ver otro tipo de errores, aquellos en los que la imagen se encuentra en el límite entre dos clases, clasificando el modelo la imagen en una clase que no es la propia. Este tipo de errores no son necesariamente preocupantes, puesto que el modelo estaría clasificando como \textit{Over} o \textit{Under} imágenes que son \textit{Ok} o al revés, pero que podrían estar en una u otra clase según el límite puesto entre estas al crear el \textit{dataset}. El resultado de este error consistiría en someter a un postprocesamiento a superficies que no lo necesitan del todo, pero que igualmente se podrían beneficiar de ello, o en ignorar en este postprocesamiento algunas imágenes que lo necesitarían, pero que no supondrían un gran fallo en el acabado de la superficie. En este caso, dependería de cada objeto que se quiera manufacturar, según cuál es el límite que se desea imponer, de forma que si la exactitud del acabado es primordial, se reduzca el número de imágenes \textit{Ok} en el \textit{dataset}, es decir, que en caso de duda entre dos clases, se decida por las clases \textit{Under} u \textit{Over} en vez de \textit{Ok}, puesto que se estará dando al modelo una mayor libertad para reconocer como estas clases, las cuales aseguran que exista un postprocesado. Al contrario, si lo que se está primando es la velocidad, se puede crear el \textit{dataset} con un mayor número de imágenes º\textit{Ok} y que tan solo se procesen algunas superficies claramente necesitadas de ello.


\begin{figure}[H]
    \centering
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/Resultados/ImMalClasif/Tipos/MalClasifLimite.png}
	 		\caption{ \textit{Over} }
                    \label{fig:MalClasificadaLimite1}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/Resultados/ImMalClasif/Tipos/MalClasifLimiteUnder.png}
	 		\caption{ \textit{Under} }
                    \label{fig:MalClasificadaLimite2}
	 	\end{subfigure}
   	 	\caption{ Imagen mal clasificada del tipo límite entre clases }
            \label{fig:MalClasificadaLimite}
\end{figure}


Otro error encontrado son las imágenes mal clasificadas, como es el caso de las de la Figura \ref{fig:MalClasificadaMal}. En estos casos, la solución es mejorar el \textit{dataset} y asegurar que las imágenes están bien clasificadas, puesto que estos errores suponen un inconveniente al obtener los resultados del modelo.


\begin{figure}[H]
\centering
            \begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/Resultados/ImMalClasif/Tipos/MalClasifMal.png}
	 		\caption{ Imagen claramente no \textit{Ok} }
                    \label{fig:MalClasificadaMal1}
	 	\end{subfigure}
	 	\begin{subfigure}[b]{0.45\linewidth}
	 	\centering
	 		\includegraphics[scale=1]{Imagenes/Resultados/ImMalClasif/Tipos/MalClasifMal2.png}
	 		\caption{ Imagen claramente no \textit{Under} }
                    \label{fig:MalClasificadaMal2}
	 	\end{subfigure}
    \caption{ Imagen mal clasificada del tipo etiqueta mal elegida  }
    \label{fig:MalClasificadaMal}
\end{figure}


Por último, también se han visto otro tipo de errores, como el que se ve en la Figura \ref{fig:MalClasificadaOtro}, donde una imagen prácticamente vacía se clasifica como \textit{Empty}, aunque al tener algo de superficie su etiqueta verdadera es otra. En este caso, la solución sería arreglar las etiquetas del \textit{dataset} como en el anterior, si bien este tipo de imagen se ha decidido meter en otro conjunto, puesto que imágenes de este tipo no supondrían ningún problema reales en el postprocesamiento, aunque sí afectan al resultado de la precisión del modelo.


\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{Imagenes/Resultados/ImMalClasif/Tipos/MalClasifOtro.png }
    \caption{ Imagen mal clasificada de otro tipo  }
    \label{fig:MalClasificadaOtro}
\end{figure}







 
\chapter{Conclusiones}




En este proyecto, se ha abordado el problema de la detección automática de artefactos o defectos en superficies fabricadas mediante manufactura aditiva utilizando redes neuronales profundas. A lo largo del estudio, se han llevado a cabo diversas etapas que han permitido obtener resultados significativos y concluyentes.

En primer lugar, se realizó una exhaustiva revisión de la literatura existente sobre la manufactura aditiva, así como de la inteligencia artificial y, en concreto, sobre las redes neuronales convolucionales. Esto proporcionó una base sólida para comprender los desafíos y enfoques previos en el campo de la detección de artefactos en la manufactura aditiva.

Posteriormente, se utilizó un conjunto de datos que fue recopilado y procesado, de forma que fuera representativo e incluyera muestras de superficies fabricadas mediante manufactura aditiva. Este conjunto de datos y su compresión fue fundamental para entender el proceso de entrenamiento de los modelos y las posibles mejoras.

Se diseñaron y entrenaron diferentes arquitecturas de redes neuronales. Se utilizaron redes neuronales preentrenadas, entendiendo sus ventajas frente a utilizar una arquitectura desde cero. También se explicaron y utilizaron conceptos para la mejora del rendimiento, como la aumentación de datos y la afinación de hiperparámetros. Estos modelos consiguieron un alto rendimiento en la detección de artefactos, demostrando las posibilidades de las redes neuronales en este campo.

Finalmente, se ha conseguido un modelo capaz de clasificar las diferentes secciones dentro de una pieza fabricada con manufactura aditiva con casi un 90\% de precisión. De esta forma, un modelo entrenado en apenas 40 minutos es capaz de establecer si las diferentes secciones de una pieza necesitan un postprocesamiento de lijado o rellenado de huecos o no.

En conclusión, este Trabajo de Fin de Grado ha demostrado la viabilidad y eficacia de utilizar redes neuronales profundas para la detección automática de artefactos en superficies fabricadas mediante manufactura aditiva. Los resultados obtenidos abren nuevas perspectivas en el campo de la calidad y control de la fabricación aditiva, brindando oportunidades para mejorar la eficiencia y precisión de los procesos de producción en esta industria.



















 
\chapter{Líneas Futuras}




A pesar de los resultados logrados en este proyecto en la detección automática de artefactos utilizando redes neuronales profundas, existen aún varias áreas de investigación y desarrollo que pueden ser exploradas en futuros estudios. A continuación, se presentan algunas posibles líneas futuras de investigación:

\begin{itemize}

\item Aumento de entrenamientos: debido a que una gran parte del tiempo en este trabajo ha sido empleado en aprender conceptos de programación y redes neuronales por parte del alumno, la posibilidad de realizar entrenamientos y más comprobaciones se ha reducido. Además, la posibilidad de entrenar los modelos se ha visto supeditada a la disponibilidad de GPU por parte de la plataforma Google Colab. Un mayor tiempo con la posibilidad de conectarse a procesadores propios permitiría la posibilidad de realizar más entrenamientos y encontrar el modelo óptimo con una mayor seguridad.

\item Mejora del \textit{dataset}: si bien en este proyecto se ha hecho todo lo posible por conseguir modelos con la máxima precisión, se han encontrado difícil de mejorar la precisión de validación llegado un límite. Un campo que, sin embargo, no se ha podido mejorar es el del conjunto de imágenes empleado. Si bien este conjunto de imágenes es amplio y ha sido clasificado con profesionalidad, no ha sido clasificado con total precisión, por lo que el modelo encuentre imágenes que no sea capaz de entender. Llevar a cabo la construcción del dataset puede permitir mejorarlo y asegurar cómo son las imágenes de las que aprende el modelo.

\item Mejora de la precisión: Aunque los modelos de redes neuronales profundas desarrollados han demostrado un alto rendimiento en la detección de artefactos, siempre existe margen para mejorar la precisión. Se pueden explorar técnicas avanzadas de preprocesamiento de datos, arquitecturas de redes neuronales más complejas y algoritmos de aprendizaje más sofisticados para mejorar la capacidad de detección y reducir los falsos positivos y falsos negativos Además, se podrían utilizar otras técnicas de optimización explicadas en la memoria.

\item Integración de técnicas de realimentación: Actualmente, los modelos de redes neuronales profundas se entrenan utilizando conjuntos de datos estáticos. Sin embargo, en entornos de fabricación aditiva en tiempo real, las condiciones pueden cambiar y los artefactos pueden evolucionar durante el proceso de fabricación. Se pueden explorar técnicas de realimentación en línea que permitan actualizar y ajustar continuamente los modelos en función de los datos en tiempo real, mejorando así la capacidad de adaptación a las variaciones del proceso de fabricación.

\item Validación experimental: Aunque este estudio ha utilizado conjuntos de datos representativos para entrenar y evaluar los modelos, es importante realizar una validación experimental en entornos de fabricación aditiva reales. Esto implicaría la implementación de los modelos en sistemas de inspección en línea y la comparación de los resultados obtenidos con inspecciones manuales y métodos de control de calidad tradicionales.

\end{itemize}


Las líneas futuras de investigación pueden incluir la mejora de la precisión de detección con un mayor número de entrenamientos, la integración de técnicas de realimentación, la mejora del conjunto de datos, la validación experimental en entornos de fabricación aditiva reales, así como el uso de otras técnicas de optimización. Estas investigaciones adicionales contribuirán a avanzar en el campo de la detección automática de artefactos y mejorar la calidad y eficiencia de los procesos de fabricación aditiva.

Dado que este proyecto ha sufrido de un tiempo limitado y de un alumno con conocimientos previos limitados, los resultados obtenidos son simples aunque buenos. El aplicar estas líneas expuestas supondría un la posibilidad de encontrar resultados aún mejores que demuestren el claro beneficio del reconocimiento de imágenes mediante redes neuronales profundas para detectar defectos en capas de manufactura aditiva.



































\appendix %aquí comienza le bloque de anexos
\clearpage
\chapter{Anexo I}
%\addcontentsline{toc}{chapter}{Anexos}

% estas tres lineas son para que aparezca el bloque anexos como bloque en la bibliografía.


\section{Códigos python}
Todos los códigos y sus resultados están disponibles en \url{https://github.com/MrMartinMLB/TFG}, así como la memoria de este proyecto, desarrollada con LaTeX, un sistema de preparación de documentos utilizado para la comunicación y publicación de documentos científicos.  A continuación se exponen algunas partes fundamentales del código.




\subsection{Librerías}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.7]{Imagenes/Codigos/Librerias.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/LibreriasComplejas.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/GPU.png}
\end{figure}
%\large{\textbf{Mostrar imágenes:}}








\subsection{Creación de dataset}
%\large{\textbf{Función de entrenamiento:}}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/Dataset1.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/Dataset2.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.8]{Imagenes/Codigos/Dataset3.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.7]{Imagenes/Codigos/Dataset4.png}
\end{figure}







\subsection{Comprobación dataset}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/CompDataset.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/CompDataset2.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/CompDataset3.png}
\end{figure}









\subsection{Definir el modelo}
%\large{\textbf{Aumentación de Datos y Dataset:}}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/DefinirModelo.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.8]{Imagenes/Codigos/DefinirModelo2.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.8]{Imagenes/Codigos/DefinirModelo3.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/DefinirModelo4.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/DefinirModelo5.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/DefinirModelo6.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.8]{Imagenes/Codigos/DefinirModelo7.png}
\end{figure}







\subsection{Entrenar el modelo}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.57]{Imagenes/Codigos/EntrenarModelo3.png}
\end{figure}






\subsection{Graficas}


\subsubsection{Gráficas de precisión y pérdidas}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/Graficar.png}
\end{figure}


\subsubsection{Matriz confusión}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/Graficar2.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/Graficar3.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/GraficarMatConf.png}
\end{figure}


\subsubsection{Curva ROC}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/GraficarOvOALL.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/GraficarOvR.png}
\end{figure}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/GraficarOvRALL.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/GraficarOvOALL2.png}
\end{figure}










\subsection{Aumentación de datos}
%\large{\textbf{Validación Cruzada:}}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/AumenDatos.png}
\end{figure}









\subsection{Hiperparámetros}
%\large{\textbf{Entrenamiento con \textit{Transfer Learning}:}}

\subsubsection{Elección de \textit{learning rate} inicial}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.57]{Imagenes/Codigos/HiperparametrosFijo.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.57]{Imagenes/Codigos/HiperparametrosFijo.png}
\end{figure}

\subsubsection{Variación de \textit{learning rate}}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.53]{Imagenes/Codigos/HiperparametrosStep.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.53]{Imagenes/Codigos/HiperparametrosExpo.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/HiperparametrosStepCambio.png}
\end{figure}








\newpage
\subsection{Validación cruzada}



\subsubsection{Aleatoria}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=1]{Imagenes/Codigos/Validacion cruzada1.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.8]{Imagenes/Codigos/Validacion cruzada2.png}
\end{figure}


\subsubsection{Series temporales}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/Validacion cruzada3.png}
\end{figure}


\subsubsection{\textit{K-folds}}


\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/Validacion cruzada4.png}
\end{figure}




\newpage

\section{Guardar y mostrar imágenes mal clasificadas}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.8]{Imagenes/Codigos/MostrarImagenes1.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/MostrarImagenes2.png}
\end{figure}

\begin{figure}[H]
    \raggedright
    \includegraphics[scale=0.9]{Imagenes/Codigos/MostrarImagenes3.png}
\end{figure}





















\newpage



%% The added code line
%\bibliographystyle{unsrt}
%\bibliography{bibliografia.bib}
%\addcontentsline{toc}{chapter}{Bibliografía}
\printbibliography
\newpage
\thispagestyle{empty}
\afterpage{\null\newpage}



\end{document}